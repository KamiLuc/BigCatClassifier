{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1liecZB2gba8"
      },
      "source": [
        "# BIG CATS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IovLJVtggint",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba32810-f92d-4960-8016-8e9319462dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9TamK0Zgbbh"
      },
      "source": [
        "### Prepare class indices and big cats species"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP_2z4rygbdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0daa89-3d73-4e53-cdf5-47054e6b8c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AFRICAN LEOPARD' 'CARACAL' 'CHEETAH' 'CLOUDED LEOPARD' 'JAGUAR' 'LIONS'\n",
            " 'OCELOT' 'PUMA' 'SNOW LEOPARD' 'TIGER']\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/BIAI/dataset/WILDCATS.CSV')\n",
        "\n",
        "indices = df.iloc[:, 0].unique()\n",
        "species = df.iloc[:, 2].unique()\n",
        "species_map = {i: big_cat_class for i, big_cat_class in zip(indices, species)}\n",
        "class_ids = df[df.iloc[:, 3] == 'train'].iloc[:, 0]\n",
        "\n",
        "print(species)\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    'valid' : transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    'test' : transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "    transforms.ToTensor()\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdhiREBsgbdg"
      },
      "source": [
        "### Load input data to arrays with appropriate transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZxIK387gbdh"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/BIAI/dataset/train/'\n",
        "valid_path = '/content/drive/MyDrive/BIAI/dataset/valid/'\n",
        "test_path = '/content/drive/MyDrive/BIAI/dataset/test/'\n",
        "\n",
        "train_images = torchvision.datasets.ImageFolder(\n",
        "    train_path, transform=data_transforms['train'])\n",
        "test_images = torchvision.datasets.ImageFolder(\n",
        "    test_path, transform=data_transforms['test'])\n",
        "valid_images = torchvision.datasets.ImageFolder(\n",
        "    valid_path, transform=data_transforms['valid'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHf-xvsggbdh"
      },
      "source": [
        "### Create data loaders to pass input data to neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_SNFVAAgbdi"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_images, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_images, batch_size=1, shuffle=False, num_workers=2)\n",
        "valid_loader = DataLoader(valid_images, batch_size=1, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e13W6Xpgbdi"
      },
      "source": [
        "### Create classifier class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgI7sLxZgbdi"
      },
      "outputs": [],
      "source": [
        "class BigCatClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BigCatClassifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.fc1 = nn.Linear(32*56*56, 256)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu3(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgYg6GfMA2Gy"
      },
      "outputs": [],
      "source": [
        "class BigCatClassifier2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BigCatClassifier2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 28 * 28, 256)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigCatClassifier3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BigCatClassifier3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 14 * 14, 256)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DGOnkJ-jdofg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unxSCY21gbdm"
      },
      "source": [
        "### Model accuracy testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgav72V0gbdm"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(log_file):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  test_correct = 0\n",
        "  valid_correct = 0\n",
        "  test_total = 0\n",
        "  valid_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          _, predicted = outputs.max(1)\n",
        "          test_total += labels.size(0)\n",
        "          test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for images, labels in valid_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          _, predicted = outputs.max(1)\n",
        "          valid_total += labels.size(0)\n",
        "          valid_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "  test_accuracy = 100 * test_correct / test_total\n",
        "  valid_accuracy = 100 * valid_correct / valid_total\n",
        "  log_file.write(f'{test_accuracy},{valid_accuracy}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv-6hGPWgbdj"
      },
      "source": [
        "### Perform classifier training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBCcYNU0gbdk"
      },
      "outputs": [],
      "source": [
        "def train(log_file):\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      loss = criterion(outputs, targets)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "    log_file.write(f'{epoch+1},{loss.item():.4f},')\n",
        "    test_accuracy(log_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kRu2e-aHFMh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgDZIO1vgbh3"
      },
      "source": [
        "### Load model from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSl1-uirgbh3"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = BigCatClassifier()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/BIAI/model.pt'))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"GPU\" if torch.cuda.is_available() else \"CPU\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNx3owJagbdn"
      },
      "source": [
        "###Auto training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMMM_kKiHSrw"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"GPU\" if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "#----------------------------------------------------\n",
        "lr = 0.001\n",
        "model = BigCatClassifier2().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "num_epochs = 140\n",
        "results_folder = \"/content/drive/MyDrive/BIAI/results/best\"\n",
        "model_name = \"nn.CrossEntropyLoss-optim.Adam-najlepszy-0.001-32-CAT2\"\n",
        "#----------------------------------------------------\n",
        "result_file = results_folder + \"/\" + model_name + \"-\" + str(batch_size) + \"-\" + str(lr) + \".result\"\n",
        "model_file = results_folder + \"/\" + model_name + \"-\" + str(batch_size) + \"-\" + str(lr) + \".pth\"\n",
        "\n",
        "with open(result_file, 'w') as file:\n",
        "  file.write(f\"batch_size: {batch_size}\\n\\n\")\n",
        "  file.write(f\"learning_rate: {lr}\\n\\n\")\n",
        "  file.write(f\"model: {str(model)} \\n\\n\")\n",
        "\n",
        "  file.write(f\"begin training\\n\\n\")\n",
        "  file.write(f\"epoch,loss,test accuracy,valid accuracy\\n\")\n",
        "  train(file)\n",
        "  file.close()\n",
        "\n",
        "torch.save(model.state_dict(), model_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate confusion matrix"
      ],
      "metadata": {
        "id": "LmRgHT5Sws76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ4Xd1bvMNBQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "\n",
        "class_labels = ['AFRICAN LEOPARD', 'CARACAL', 'CHEETAH', 'CLOUDED LEOPARD', 'JAGUAR', 'LIONS', 'OCELOT', 'PUMA', 'SNOW LEOPARD', 'TIGER']\n",
        "\n",
        "test_correct = 0\n",
        "valid_correct = 0\n",
        "test_total = 0\n",
        "valid_total = 0\n",
        "\n",
        "real_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += predicted.eq(labels).sum().item()\n",
        "        real_labels.extend(labels.cpu().numpy())\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    conf_matrix = confusion_matrix(real_labels, predicted_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
        "\n",
        "    plt.colorbar()\n",
        "    classes = np.arange(len(class_labels))\n",
        "    plt.xticks(classes, class_labels, rotation=90)\n",
        "    plt.yticks(classes, class_labels)\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('Valid Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}