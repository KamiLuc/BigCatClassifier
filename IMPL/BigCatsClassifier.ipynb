{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIG CATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('dataset/WILDCATS.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare class indices and big cats species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df.iloc[:, 0].unique()\n",
    "species = df.iloc[:, 2].unique()\n",
    "species_map = {i: big_cat_class for i, big_cat_class in zip(indices, species)}\n",
    "class_ids = df[df.iloc[:, 3] == 'train'].iloc[:, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'valid' : transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test' : transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input data to arrays with appropriate transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dataset/train/'\n",
    "valid_path = 'dataset/valid/'\n",
    "test_path = 'dataset/test/'\n",
    "\n",
    "train_images = torchvision.datasets.ImageFolder(\n",
    "    train_path, transform=data_transforms['train'])\n",
    "test_images = torchvision.datasets.ImageFolder(\n",
    "    test_path, transform=data_transforms['test'])\n",
    "valid_images = torchvision.datasets.ImageFolder(\n",
    "    valid_path, transform=data_transforms['valid'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders to pass input data to neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_images, batch_size=1, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_images, batch_size=1, shuffle=False, num_workers=0)\n",
    "valid_loader = DataLoader(valid_images, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigCatClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BigCatClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)  # Warstwa konwolucyjna\n",
    "        self.relu1 = nn.ReLU()  # Warstwa aktywacji ReLU\n",
    "        self.flatten = nn.Flatten()  # Warstwa spłaszczająca dane\n",
    "        self.fc1 = nn.Linear(16*222*222, 10)  # Warstwa w pełni połączona\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    id = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/2339], Loss: 2.7680\n",
      "Epoch [1/20], Step [200/2339], Loss: 2.0546\n",
      "Epoch [1/20], Step [300/2339], Loss: 1.8787\n",
      "Epoch [1/20], Step [400/2339], Loss: 2.4399\n",
      "Epoch [1/20], Step [500/2339], Loss: 2.6579\n",
      "Epoch [1/20], Step [600/2339], Loss: 3.3131\n",
      "Epoch [1/20], Step [700/2339], Loss: 2.9954\n",
      "Epoch [1/20], Step [800/2339], Loss: 2.3527\n",
      "Epoch [1/20], Step [900/2339], Loss: 2.0314\n",
      "Epoch [1/20], Step [1000/2339], Loss: 2.6665\n",
      "Epoch [1/20], Step [1100/2339], Loss: 3.0162\n",
      "Epoch [1/20], Step [1200/2339], Loss: 2.2299\n",
      "Epoch [1/20], Step [1300/2339], Loss: 2.5080\n",
      "Epoch [1/20], Step [1400/2339], Loss: 2.4423\n",
      "Epoch [1/20], Step [1500/2339], Loss: 2.9617\n",
      "Epoch [1/20], Step [1600/2339], Loss: 2.7076\n",
      "Epoch [1/20], Step [1700/2339], Loss: 2.4555\n",
      "Epoch [1/20], Step [1800/2339], Loss: 2.7491\n",
      "Epoch [1/20], Step [1900/2339], Loss: 2.1013\n",
      "Epoch [1/20], Step [2000/2339], Loss: 2.5210\n",
      "Epoch [1/20], Step [2100/2339], Loss: 2.5768\n",
      "Epoch [1/20], Step [2200/2339], Loss: 2.2273\n",
      "Epoch [1/20], Step [2300/2339], Loss: 2.2108\n",
      "Dokładność na danych testowych: 10.00%\n",
      "Dokładność na danych walidacyjnych: 10.00%\n",
      "Trenowanie zakończone.\n",
      "Epoch [2/20], Step [100/2339], Loss: 2.8390\n",
      "Epoch [2/20], Step [200/2339], Loss: 2.9777\n",
      "Epoch [2/20], Step [300/2339], Loss: 2.9785\n",
      "Epoch [2/20], Step [400/2339], Loss: 2.8228\n",
      "Epoch [2/20], Step [500/2339], Loss: 2.5345\n",
      "Epoch [2/20], Step [600/2339], Loss: 2.6382\n",
      "Epoch [2/20], Step [700/2339], Loss: 2.5466\n",
      "Epoch [2/20], Step [800/2339], Loss: 3.2235\n",
      "Epoch [2/20], Step [900/2339], Loss: 2.3742\n",
      "Epoch [2/20], Step [1000/2339], Loss: 2.5292\n",
      "Epoch [2/20], Step [1100/2339], Loss: 2.4753\n",
      "Epoch [2/20], Step [1200/2339], Loss: 2.5347\n",
      "Epoch [2/20], Step [1300/2339], Loss: 1.8561\n",
      "Epoch [2/20], Step [1400/2339], Loss: 2.1520\n",
      "Epoch [2/20], Step [1500/2339], Loss: 2.2634\n",
      "Epoch [2/20], Step [1600/2339], Loss: 2.8338\n",
      "Epoch [2/20], Step [1700/2339], Loss: 1.9784\n",
      "Epoch [2/20], Step [1800/2339], Loss: 1.9537\n",
      "Epoch [2/20], Step [1900/2339], Loss: 1.8269\n",
      "Epoch [2/20], Step [2000/2339], Loss: 1.6843\n",
      "Epoch [2/20], Step [2100/2339], Loss: 2.0978\n",
      "Epoch [2/20], Step [2200/2339], Loss: 1.9278\n",
      "Epoch [2/20], Step [2300/2339], Loss: 2.3423\n",
      "Dokładność na danych testowych: 10.00%\n",
      "Dokładność na danych walidacyjnych: 10.00%\n",
      "Trenowanie zakończone.\n",
      "Epoch [3/20], Step [100/2339], Loss: 2.5330\n",
      "Epoch [3/20], Step [200/2339], Loss: 2.3381\n",
      "Epoch [3/20], Step [300/2339], Loss: 2.2955\n",
      "Epoch [3/20], Step [400/2339], Loss: 3.2077\n",
      "Epoch [3/20], Step [500/2339], Loss: 1.7333\n",
      "Epoch [3/20], Step [600/2339], Loss: 1.7255\n",
      "Epoch [3/20], Step [700/2339], Loss: 2.6274\n",
      "Epoch [3/20], Step [800/2339], Loss: 2.7270\n",
      "Epoch [3/20], Step [900/2339], Loss: 2.5844\n",
      "Epoch [3/20], Step [1000/2339], Loss: 1.8664\n",
      "Epoch [3/20], Step [1100/2339], Loss: 2.1477\n",
      "Epoch [3/20], Step [1200/2339], Loss: 2.7416\n",
      "Epoch [3/20], Step [1300/2339], Loss: 2.4433\n",
      "Epoch [3/20], Step [1400/2339], Loss: 2.3791\n",
      "Epoch [3/20], Step [1500/2339], Loss: 2.1828\n",
      "Epoch [3/20], Step [1600/2339], Loss: 3.2867\n",
      "Epoch [3/20], Step [1700/2339], Loss: 1.9093\n",
      "Epoch [3/20], Step [1800/2339], Loss: 1.7281\n",
      "Epoch [3/20], Step [1900/2339], Loss: 1.7075\n",
      "Epoch [3/20], Step [2000/2339], Loss: 2.2150\n",
      "Epoch [3/20], Step [2100/2339], Loss: 2.1169\n",
      "Epoch [3/20], Step [2200/2339], Loss: 2.2288\n",
      "Epoch [3/20], Step [2300/2339], Loss: 1.7266\n",
      "Dokładność na danych testowych: 10.00%\n",
      "Dokładność na danych walidacyjnych: 10.00%\n",
      "Trenowanie zakończone.\n",
      "Epoch [4/20], Step [100/2339], Loss: 1.6096\n",
      "Epoch [4/20], Step [200/2339], Loss: 2.0805\n",
      "Epoch [4/20], Step [300/2339], Loss: 2.6372\n",
      "Epoch [4/20], Step [400/2339], Loss: 2.3379\n",
      "Epoch [4/20], Step [500/2339], Loss: 3.0964\n",
      "Epoch [4/20], Step [600/2339], Loss: 2.7438\n",
      "Epoch [4/20], Step [700/2339], Loss: 2.3827\n",
      "Epoch [4/20], Step [800/2339], Loss: 2.1741\n",
      "Epoch [4/20], Step [900/2339], Loss: 1.7570\n",
      "Epoch [4/20], Step [1000/2339], Loss: 2.5210\n",
      "Epoch [4/20], Step [1100/2339], Loss: 2.4150\n",
      "Epoch [4/20], Step [1200/2339], Loss: 2.5642\n",
      "Epoch [4/20], Step [1300/2339], Loss: 2.8536\n",
      "Epoch [4/20], Step [1400/2339], Loss: 2.0604\n",
      "Epoch [4/20], Step [1500/2339], Loss: 2.2448\n",
      "Epoch [4/20], Step [1600/2339], Loss: 2.8329\n",
      "Epoch [4/20], Step [1700/2339], Loss: 2.7026\n",
      "Epoch [4/20], Step [1800/2339], Loss: 2.3340\n",
      "Epoch [4/20], Step [1900/2339], Loss: 2.6289\n",
      "Epoch [4/20], Step [2000/2339], Loss: 2.7127\n",
      "Epoch [4/20], Step [2100/2339], Loss: 2.4721\n",
      "Epoch [4/20], Step [2200/2339], Loss: 2.6978\n",
      "Epoch [4/20], Step [2300/2339], Loss: 2.4772\n",
      "Dokładność na danych testowych: 10.00%\n",
      "Dokładność na danych walidacyjnych: 10.00%\n",
      "Trenowanie zakończone.\n",
      "Epoch [5/20], Step [100/2339], Loss: 2.3460\n",
      "Epoch [5/20], Step [200/2339], Loss: 1.9903\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BigCatClassifier()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zeruj gradienty\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Przebieg w przód\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Oblicz funkcję straty\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Propagacja wsteczna\n",
    "        loss.backward()\n",
    "\n",
    "        # Aktualizacja wag\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\n",
    "                f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "    model.eval()\n",
    "\n",
    "    # Inicjalizacja zmiennych do przechowywania wyników\n",
    "    test_correct = 0\n",
    "    valid_correct = 0\n",
    "    test_total = 0\n",
    "    valid_total = 0\n",
    "\n",
    "    # Ocena modelu na danych testowych\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Ocena modelu na danych walidacyjnych\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            valid_total += labels.size(0)\n",
    "            valid_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "\n",
    "    print('Dokładność na danych testowych: {:.2f}%'.format(test_accuracy))\n",
    "    print('Dokładność na danych walidacyjnych: {:.2f}%'.format(valid_accuracy))\n",
    "\n",
    "    print('Trenowanie zakończone.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność na danych testowych: 44.00%\n",
      "Dokładność na danych walidacyjnych: 48.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "# Inicjalizacja zmiennych do przechowywania wyników\n",
    "test_correct = 0\n",
    "valid_correct = 0\n",
    "test_total = 0\n",
    "valid_total = 0\n",
    "\n",
    "# Ocena modelu na danych testowych\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "# Ocena modelu na danych walidacyjnych\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        valid_total += labels.size(0)\n",
    "        valid_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "valid_accuracy = 100 * valid_correct / valid_total\n",
    "\n",
    "print('Dokładność na danych testowych: {:.2f}%'.format(test_accuracy))\n",
    "print('Dokładność na danych walidacyjnych: {:.2f}%'.format(valid_accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BigCatClassifier' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m model\u001b[39m.\u001b[39;49mid)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BigCatClassifier' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigCatClassifier(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=788544, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = BigCatClassifier()\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
