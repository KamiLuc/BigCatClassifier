batch_size: 64

learning_rate: 0.001

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3030,10.0,10.0
2,2.3121,10.0,14.0
3,2.2925,10.0,12.0
4,2.3169,16.0,20.0
5,2.3014,10.0,22.0
6,2.2972,14.0,20.0
7,2.2812,14.0,18.0
8,2.2875,18.0,22.0
9,2.2862,18.0,26.0
10,2.3132,18.0,16.0
11,2.2642,18.0,26.0
12,2.2424,14.0,22.0
13,2.1826,14.0,24.0
14,2.2208,16.0,30.0
15,2.1392,22.0,26.0
16,2.1707,18.0,28.0
17,2.1547,20.0,32.0
18,2.0848,22.0,36.0
19,2.0236,18.0,38.0
20,1.7596,10.0,24.0
21,1.7932,26.0,36.0
22,2.0218,20.0,22.0
23,1.7534,28.0,40.0
24,1.8344,26.0,44.0
25,1.5489,16.0,42.0
26,1.5559,30.0,50.0
27,1.8213,30.0,50.0
28,1.5840,24.0,36.0
29,1.6076,30.0,44.0
30,1.6349,36.0,50.0
31,1.6757,26.0,42.0
32,1.4050,36.0,44.0
33,1.4753,42.0,50.0
34,1.3276,32.0,46.0
35,1.3008,44.0,48.0
36,1.7318,38.0,46.0
37,1.4877,32.0,50.0
38,1.5270,30.0,48.0
39,1.8724,38.0,44.0
40,1.5431,38.0,46.0
41,1.0855,48.0,50.0
42,1.3664,48.0,54.0
43,1.2826,38.0,48.0
44,1.6372,42.0,54.0
45,1.7137,40.0,46.0
46,1.4019,46.0,46.0
47,1.2934,36.0,46.0
48,1.2393,40.0,42.0
49,1.2995,50.0,44.0
50,1.1672,44.0,42.0
51,1.6195,46.0,56.0
52,1.4354,48.0,52.0
53,1.3577,54.0,58.0
54,1.2107,40.0,50.0
55,1.0532,50.0,58.0
56,1.0866,48.0,56.0
57,1.3289,58.0,52.0
58,1.5006,46.0,46.0
59,1.0576,50.0,50.0
60,1.1855,56.0,50.0
61,1.2491,54.0,56.0
62,0.8858,62.0,54.0
63,1.0771,56.0,48.0
64,1.1749,58.0,56.0
65,1.2562,56.0,58.0
66,0.9890,50.0,60.0
67,1.0015,66.0,50.0
68,1.0590,62.0,56.0
69,0.9589,60.0,44.0
70,0.9873,64.0,50.0
71,1.1636,60.0,60.0
72,0.8563,60.0,58.0
73,1.1405,62.0,58.0
74,0.9771,60.0,54.0
75,1.3109,60.0,60.0
76,0.8157,66.0,52.0
77,0.8377,58.0,60.0
78,0.9997,64.0,60.0
79,0.8453,60.0,66.0
80,0.9031,60.0,60.0
81,0.9744,62.0,58.0
82,1.0762,58.0,60.0
83,1.1057,66.0,58.0
84,0.9004,60.0,64.0
85,0.7803,66.0,62.0
86,0.7851,58.0,60.0
87,0.9354,62.0,62.0
88,0.8441,60.0,64.0
89,0.4670,66.0,58.0
90,0.5084,68.0,62.0
91,0.7311,60.0,60.0
92,0.6848,60.0,58.0
93,0.8058,74.0,66.0
94,0.6475,66.0,68.0
95,1.0168,64.0,70.0
96,0.8302,56.0,76.0
97,0.5041,64.0,70.0
98,0.7744,68.0,68.0
99,0.7722,68.0,70.0
100,0.4291,64.0,58.0
