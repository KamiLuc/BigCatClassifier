batch_size: 32

learning_rate: 0.001

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3068,10.0,18.0
2,2.2862,14.0,28.0
3,2.2502,14.0,22.0
4,2.2117,16.0,22.0
5,2.2188,14.0,24.0
6,2.3048,12.0,30.0
7,1.8516,14.0,18.0
8,1.9709,18.0,36.0
9,2.2075,20.0,48.0
10,2.1797,28.0,48.0
11,1.6140,22.0,36.0
12,1.7644,10.0,24.0
13,1.3719,14.0,30.0
14,1.1957,30.0,48.0
15,1.1944,32.0,46.0
16,1.1342,24.0,36.0
17,1.8388,34.0,38.0
18,1.1945,34.0,48.0
19,1.5051,32.0,46.0
20,0.8734,34.0,50.0
21,1.1386,26.0,42.0
22,1.2064,36.0,46.0
23,1.2462,26.0,44.0
24,1.4354,36.0,46.0
25,1.4351,26.0,36.0
26,1.9518,36.0,50.0
27,1.5430,42.0,48.0
28,1.4123,44.0,56.0
29,1.2472,40.0,46.0
30,1.6201,44.0,54.0
31,0.6347,42.0,54.0
32,2.2859,44.0,48.0
33,1.0435,42.0,56.0
34,0.7857,36.0,54.0
35,2.0279,38.0,46.0
36,1.4996,50.0,46.0
37,0.6873,38.0,54.0
38,0.4786,44.0,52.0
39,1.2603,60.0,54.0
40,1.2339,50.0,58.0
41,0.3551,56.0,52.0
42,2.8359,62.0,52.0
43,1.6018,60.0,48.0
44,1.8093,56.0,48.0
45,0.7150,46.0,54.0
46,1.6273,56.0,56.0
47,1.0532,60.0,52.0
48,1.1408,56.0,50.0
49,1.7679,44.0,56.0
50,0.8109,50.0,62.0
51,2.3349,60.0,56.0
52,1.2658,52.0,56.0
53,1.9280,58.0,52.0
54,0.9069,52.0,60.0
55,1.6630,60.0,56.0
56,1.4056,54.0,60.0
57,0.6433,68.0,64.0
58,0.2701,68.0,60.0
59,1.0809,60.0,58.0
60,0.6248,68.0,58.0
61,0.5855,62.0,64.0
62,0.1993,64.0,60.0
63,0.4143,64.0,56.0
64,0.9224,52.0,60.0
65,0.2874,60.0,64.0
66,1.5320,58.0,54.0
67,0.1182,60.0,68.0
68,0.5840,68.0,64.0
69,0.4905,64.0,68.0
70,0.3462,70.0,60.0
71,0.2203,68.0,64.0
72,0.5952,66.0,60.0
73,0.7138,54.0,62.0
74,1.2532,58.0,58.0
75,0.2894,70.0,66.0
76,0.0899,76.0,66.0
77,0.3296,66.0,70.0
78,0.2071,66.0,66.0
79,1.2628,68.0,66.0
80,1.8395,54.0,64.0
81,0.5600,58.0,66.0
82,0.5414,68.0,64.0
83,0.1652,58.0,66.0
84,0.0474,66.0,64.0
85,1.7924,64.0,66.0
86,0.5057,60.0,62.0
87,0.4693,66.0,66.0
88,0.1322,62.0,66.0
89,1.1407,68.0,62.0
90,0.7130,68.0,68.0
91,0.0822,66.0,64.0
92,0.1417,70.0,68.0
93,0.0950,66.0,66.0
94,1.7748,54.0,62.0
95,0.7789,74.0,68.0
96,0.4955,60.0,64.0
97,0.3600,64.0,64.0
98,0.0441,72.0,64.0
99,0.0570,62.0,70.0
100,0.1430,68.0,64.0
