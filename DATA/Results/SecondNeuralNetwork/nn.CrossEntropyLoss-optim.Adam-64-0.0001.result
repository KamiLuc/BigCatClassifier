batch_size: 64

learning_rate: 0.0001

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2573,22.0,24.0
2,2.1925,24.0,34.0
3,2.1402,26.0,44.0
4,1.8634,22.0,40.0
5,1.7346,30.0,46.0
6,1.7358,32.0,50.0
7,1.8563,30.0,48.0
8,1.6808,26.0,40.0
9,1.5151,32.0,46.0
10,1.6411,26.0,46.0
11,1.5952,40.0,46.0
12,1.6951,32.0,48.0
13,1.7581,32.0,48.0
14,1.4390,42.0,46.0
15,1.5274,42.0,48.0
16,1.4588,42.0,42.0
17,1.4242,36.0,40.0
18,1.6532,48.0,46.0
19,1.6484,50.0,52.0
20,1.5328,44.0,52.0
21,1.1343,42.0,50.0
22,1.3790,40.0,52.0
23,1.5638,38.0,50.0
24,1.4111,56.0,50.0
25,1.4230,52.0,52.0
26,1.1539,58.0,48.0
27,1.3512,46.0,50.0
28,1.5120,52.0,50.0
29,1.3698,56.0,52.0
30,1.5618,52.0,52.0
31,1.4022,54.0,54.0
32,1.3024,56.0,52.0
33,1.3325,50.0,56.0
34,1.4314,52.0,56.0
35,1.1754,60.0,60.0
36,0.9625,52.0,50.0
37,1.1172,56.0,60.0
38,1.3321,60.0,54.0
39,1.0707,52.0,58.0
40,1.0404,56.0,54.0
41,1.1350,60.0,58.0
42,1.1742,54.0,54.0
43,1.0501,56.0,60.0
44,0.9364,56.0,60.0
45,1.0809,62.0,58.0
46,1.3278,52.0,60.0
47,0.9535,64.0,60.0
48,0.9048,58.0,60.0
49,0.8222,64.0,54.0
50,1.2154,58.0,56.0
51,1.0080,64.0,56.0
52,0.9772,58.0,60.0
53,1.0190,62.0,60.0
54,1.0053,60.0,60.0
55,0.8404,58.0,60.0
56,0.9495,64.0,60.0
57,0.7948,68.0,62.0
58,1.0942,64.0,56.0
59,0.7967,66.0,56.0
60,0.7069,62.0,60.0
61,0.7152,60.0,62.0
62,0.8196,60.0,56.0
63,0.7456,60.0,62.0
64,0.8510,62.0,62.0
65,0.8586,58.0,56.0
66,0.8196,60.0,60.0
67,0.6422,62.0,54.0
68,0.5838,64.0,58.0
69,0.7759,68.0,60.0
70,0.8528,62.0,62.0
71,0.9430,56.0,56.0
72,0.8682,66.0,52.0
73,0.6649,64.0,54.0
74,0.6891,60.0,60.0
75,0.6659,62.0,58.0
76,0.8088,66.0,62.0
77,0.6716,62.0,54.0
78,0.7634,62.0,54.0
79,0.6304,66.0,58.0
80,0.7348,66.0,62.0
81,0.9511,62.0,60.0
82,0.5535,68.0,58.0
83,0.8815,62.0,58.0
84,0.6628,66.0,60.0
85,0.5689,62.0,58.0
86,0.6830,68.0,54.0
87,0.7919,68.0,58.0
88,0.7207,66.0,58.0
89,0.7838,68.0,62.0
90,0.6236,70.0,50.0
91,0.6519,68.0,62.0
92,0.5127,64.0,66.0
93,0.6597,68.0,60.0
94,0.7432,66.0,60.0
95,0.5324,68.0,58.0
96,0.6695,64.0,62.0
97,0.5771,62.0,56.0
98,0.8890,64.0,54.0
99,0.4078,60.0,56.0
100,0.3882,66.0,56.0
101,0.5166,66.0,58.0
102,0.5169,66.0,58.0
103,0.3644,70.0,58.0
104,0.3203,72.0,60.0
105,0.4736,72.0,54.0
106,0.4652,68.0,62.0
107,0.5310,66.0,62.0
108,0.5862,58.0,60.0
109,0.4820,64.0,60.0
110,0.4598,68.0,60.0
111,0.4337,66.0,58.0
112,0.4687,68.0,56.0
113,0.3812,68.0,66.0
114,0.5447,70.0,60.0
115,0.4666,68.0,54.0
116,0.4570,62.0,60.0
117,0.4130,66.0,58.0
118,0.6658,60.0,56.0
119,0.4952,66.0,56.0
120,0.4453,68.0,64.0
121,0.4495,72.0,62.0
122,0.4628,66.0,60.0
123,0.5097,68.0,58.0
124,0.5760,70.0,56.0
125,0.4679,64.0,60.0
126,0.3182,70.0,62.0
127,0.4751,64.0,56.0
128,0.3986,70.0,62.0
129,0.6320,66.0,58.0
130,0.3905,72.0,62.0
131,0.3357,72.0,62.0
132,0.4576,70.0,62.0
133,0.4213,70.0,62.0
134,0.2255,62.0,64.0
135,0.2577,66.0,66.0
136,0.7677,68.0,64.0
137,0.3870,70.0,62.0
138,0.4055,70.0,60.0
139,0.3438,70.0,56.0
140,0.3549,72.0,64.0
141,0.6427,66.0,64.0
142,0.3461,64.0,58.0
143,0.5340,72.0,58.0
144,0.4408,70.0,58.0
145,0.1647,72.0,60.0
146,0.2663,64.0,60.0
147,0.2199,70.0,60.0
148,0.4145,72.0,62.0
149,0.4068,66.0,60.0
150,0.4960,70.0,60.0
