batch_size: 128

learning_rate: 0.01

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3175,16.0,16.0
2,2.3027,14.0,18.0
3,2.2986,20.0,26.0
4,2.3077,18.0,20.0
5,2.1676,14.0,30.0
6,2.0023,22.0,18.0
7,1.8026,14.0,30.0
8,1.8285,20.0,40.0
9,1.5870,24.0,36.0
10,1.6227,30.0,48.0
11,1.7305,26.0,30.0
12,1.5275,36.0,38.0
13,1.5907,28.0,40.0
14,1.4546,52.0,48.0
15,1.2885,42.0,54.0
16,1.2875,44.0,56.0
17,1.4014,58.0,52.0
18,1.4050,42.0,46.0
19,1.4854,54.0,46.0
20,1.3655,42.0,48.0
21,1.1938,56.0,58.0
22,1.1062,58.0,52.0
23,1.1734,52.0,54.0
24,1.2214,56.0,52.0
25,1.3398,58.0,56.0
26,0.8812,48.0,48.0
27,0.9423,56.0,52.0
28,1.0596,58.0,66.0
29,1.0748,58.0,56.0
30,0.8933,64.0,62.0
31,0.5802,56.0,48.0
32,0.6649,66.0,52.0
33,1.0467,76.0,58.0
34,1.0874,54.0,56.0
35,0.9064,70.0,64.0
36,0.5416,78.0,54.0
37,0.5850,70.0,60.0
38,0.6281,72.0,64.0
39,0.6934,66.0,60.0
40,0.9224,68.0,60.0
41,0.7779,74.0,56.0
42,0.7240,70.0,60.0
43,0.4237,70.0,58.0
44,0.4937,84.0,60.0
45,0.6463,76.0,64.0
46,0.6361,66.0,62.0
47,0.4663,78.0,64.0
48,0.5065,80.0,62.0
49,0.5519,68.0,56.0
50,0.6123,72.0,68.0
51,0.3180,64.0,70.0
52,0.3279,82.0,62.0
53,0.3777,76.0,58.0
54,0.3655,78.0,68.0
55,0.4252,84.0,66.0
56,0.4181,76.0,68.0
57,0.3882,78.0,72.0
58,0.4963,78.0,68.0
59,0.3567,76.0,66.0
60,0.6764,78.0,68.0
61,0.4496,72.0,70.0
62,0.3117,76.0,72.0
63,0.3724,74.0,68.0
64,0.3699,74.0,66.0
65,0.4498,70.0,64.0
66,0.4396,74.0,66.0
67,0.3234,78.0,68.0
68,0.1295,72.0,62.0
69,0.0997,70.0,64.0
70,0.1126,78.0,72.0
71,0.1238,80.0,66.0
72,0.1070,80.0,72.0
73,0.1512,76.0,74.0
74,0.1347,82.0,66.0
75,0.1684,74.0,70.0
76,0.3327,84.0,72.0
77,0.2781,88.0,70.0
78,0.1779,74.0,66.0
79,0.1626,74.0,66.0
80,0.0745,80.0,66.0
81,0.1477,74.0,72.0
82,0.1067,86.0,62.0
83,0.1434,74.0,70.0
84,0.1356,76.0,66.0
85,0.0586,76.0,72.0
86,0.0590,84.0,64.0
87,0.1523,78.0,70.0
88,0.1365,80.0,74.0
89,0.0645,82.0,66.0
90,0.0244,68.0,66.0
91,0.0973,82.0,72.0
92,0.0886,76.0,64.0
93,0.2464,84.0,64.0
94,0.0955,78.0,72.0
95,0.1112,72.0,68.0
96,0.1576,76.0,72.0
97,0.0820,78.0,70.0
98,0.0826,78.0,68.0
99,0.1037,82.0,72.0
100,0.2968,74.0,72.0
