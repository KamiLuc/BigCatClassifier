batch_size: 128

learning_rate: 0.0001

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3032,14.0,8.0
2,2.3025,14.0,8.0
3,2.3014,14.0,8.0
4,2.3000,14.0,8.0
5,2.3018,12.0,6.0
6,2.2929,18.0,10.0
7,2.3075,18.0,12.0
8,2.2998,14.0,12.0
9,2.2972,12.0,10.0
10,2.2975,12.0,10.0
11,2.3034,12.0,10.0
12,2.3086,10.0,10.0
13,2.2992,10.0,10.0
14,2.3017,10.0,10.0
15,2.3062,10.0,10.0
16,2.2999,10.0,10.0
17,2.2989,10.0,10.0
18,2.3043,10.0,10.0
19,2.2978,10.0,10.0
20,2.2956,10.0,10.0
21,2.3001,10.0,10.0
22,2.2930,10.0,10.0
23,2.2911,10.0,10.0
24,2.3070,10.0,10.0
25,2.2957,10.0,10.0
26,2.2918,10.0,10.0
27,2.3069,10.0,10.0
28,2.2975,10.0,10.0
29,2.3019,10.0,10.0
30,2.2950,10.0,10.0
31,2.2992,10.0,10.0
32,2.2917,10.0,10.0
33,2.2934,10.0,10.0
34,2.2906,10.0,10.0
35,2.3103,10.0,10.0
36,2.2909,10.0,10.0
37,2.3016,10.0,10.0
38,2.3006,10.0,14.0
39,2.2910,10.0,12.0
40,2.3031,10.0,16.0
41,2.3020,10.0,16.0
42,2.2961,10.0,16.0
43,2.2964,10.0,16.0
44,2.3064,12.0,18.0
45,2.2950,12.0,20.0
46,2.3025,12.0,20.0
47,2.2919,12.0,20.0
48,2.2992,14.0,22.0
49,2.2910,12.0,22.0
50,2.3096,14.0,22.0
51,2.2903,14.0,22.0
52,2.3015,16.0,22.0
53,2.2874,16.0,22.0
54,2.2998,16.0,24.0
55,2.2941,16.0,22.0
56,2.2812,16.0,24.0
57,2.2907,16.0,24.0
58,2.3043,16.0,24.0
59,2.2957,16.0,24.0
60,2.2947,16.0,24.0
61,2.2984,16.0,24.0
62,2.2952,16.0,24.0
63,2.2925,16.0,24.0
64,2.2819,16.0,24.0
65,2.2892,16.0,24.0
66,2.3094,16.0,24.0
67,2.2925,16.0,24.0
68,2.2949,16.0,24.0
69,2.2890,16.0,24.0
70,2.2990,16.0,24.0
71,2.2802,18.0,24.0
72,2.2998,18.0,26.0
73,2.2868,18.0,26.0
74,2.2886,18.0,26.0
75,2.2935,20.0,28.0
76,2.2928,18.0,26.0
77,2.2838,18.0,26.0
78,2.2918,18.0,28.0
79,2.2889,18.0,28.0
80,2.2847,18.0,26.0
81,2.2829,18.0,26.0
82,2.2877,18.0,26.0
83,2.2777,18.0,26.0
84,2.2731,22.0,28.0
85,2.2878,18.0,28.0
86,2.2904,22.0,28.0
87,2.2874,26.0,28.0
88,2.2792,26.0,28.0
89,2.2772,22.0,28.0
90,2.2991,18.0,28.0
91,2.2959,18.0,28.0
92,2.2893,18.0,28.0
93,2.2984,16.0,28.0
94,2.2776,18.0,28.0
95,2.2901,18.0,26.0
96,2.2974,18.0,26.0
97,2.2768,18.0,26.0
98,2.2857,20.0,26.0
99,2.2759,18.0,28.0
100,2.2730,20.0,28.0
