batch_size: 128

learning_rate: 0.001

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2952,10.0,20.0
2,2.2962,12.0,12.0
3,2.2912,8.0,12.0
4,2.2837,12.0,10.0
5,2.3121,8.0,12.0
6,2.3021,10.0,14.0
7,2.2987,10.0,12.0
8,2.2860,14.0,12.0
9,2.2736,10.0,20.0
10,2.2772,10.0,16.0
11,2.2938,12.0,20.0
12,2.2738,12.0,10.0
13,2.2665,12.0,18.0
14,2.2914,12.0,20.0
15,2.2623,10.0,16.0
16,2.2964,16.0,14.0
17,2.2389,12.0,14.0
18,2.2898,20.0,26.0
19,2.2569,16.0,14.0
20,2.2724,18.0,14.0
21,2.2104,22.0,14.0
22,2.2077,18.0,22.0
23,2.1721,24.0,16.0
24,2.1670,16.0,14.0
25,2.1533,24.0,20.0
26,2.0989,18.0,22.0
27,2.1568,22.0,22.0
28,2.1246,10.0,14.0
29,2.1196,24.0,20.0
30,2.1251,24.0,30.0
31,2.0020,16.0,34.0
32,2.0490,22.0,28.0
33,1.9871,22.0,38.0
34,1.9125,24.0,36.0
35,1.9247,22.0,30.0
36,1.7903,26.0,34.0
37,1.8734,28.0,36.0
38,1.7742,32.0,40.0
39,1.7131,26.0,42.0
40,1.6653,20.0,32.0
41,1.6221,26.0,38.0
42,1.6574,28.0,42.0
43,1.6377,20.0,34.0
44,1.6604,26.0,40.0
45,1.6247,32.0,46.0
46,1.6487,28.0,44.0
47,1.6017,32.0,46.0
48,1.7842,28.0,44.0
49,1.7726,22.0,40.0
50,1.4690,36.0,48.0
51,1.4583,34.0,46.0
52,1.5181,36.0,50.0
53,1.7538,30.0,46.0
54,1.5798,42.0,46.0
55,1.6726,38.0,50.0
56,1.6208,38.0,46.0
57,1.4524,32.0,54.0
58,1.4993,30.0,48.0
59,1.2589,34.0,46.0
60,1.5162,32.0,46.0
61,1.4710,38.0,42.0
62,1.3591,40.0,48.0
63,1.3999,46.0,52.0
64,1.4094,42.0,46.0
65,1.3170,44.0,50.0
66,1.4225,44.0,50.0
67,1.3983,46.0,52.0
68,1.3662,48.0,48.0
69,1.4211,50.0,54.0
70,1.3004,46.0,52.0
71,1.5366,40.0,56.0
72,1.3396,40.0,50.0
73,1.2341,40.0,50.0
74,1.4743,42.0,48.0
75,1.1528,42.0,46.0
76,1.3273,46.0,48.0
77,1.3226,42.0,58.0
78,1.2558,42.0,54.0
79,1.1842,48.0,54.0
80,1.1343,48.0,56.0
81,1.3867,36.0,42.0
82,1.3074,50.0,56.0
83,1.3875,54.0,56.0
84,1.3083,54.0,58.0
85,1.3300,48.0,54.0
86,1.3102,50.0,52.0
87,1.2802,48.0,56.0
88,1.2003,50.0,58.0
89,1.3256,54.0,52.0
90,1.4319,48.0,52.0
91,1.1477,58.0,56.0
92,1.2244,52.0,62.0
93,1.2168,62.0,62.0
94,1.1373,48.0,50.0
95,1.1438,58.0,56.0
96,1.2280,62.0,58.0
97,1.1201,56.0,58.0
98,1.5495,56.0,60.0
99,1.4328,64.0,56.0
100,1.0109,62.0,62.0
