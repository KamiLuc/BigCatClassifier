batch_size: 128

learning_rate: 0.0001

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2781,14.0,18.0
2,2.2109,16.0,24.0
3,2.1451,26.0,36.0
4,2.2293,18.0,28.0
5,1.9591,30.0,36.0
6,1.8135,30.0,44.0
7,1.6173,24.0,40.0
8,1.6163,38.0,46.0
9,1.7824,28.0,40.0
10,1.5181,24.0,40.0
11,1.4998,30.0,46.0
12,1.6954,32.0,46.0
13,1.6031,38.0,48.0
14,1.6466,36.0,48.0
15,1.7152,28.0,36.0
16,1.4441,38.0,36.0
17,1.5271,34.0,40.0
18,1.4684,42.0,36.0
19,1.6704,38.0,48.0
20,1.5486,42.0,40.0
21,1.4758,42.0,44.0
22,1.3840,50.0,36.0
23,1.5818,46.0,44.0
24,1.4477,46.0,42.0
25,1.3848,46.0,46.0
26,1.4258,56.0,44.0
27,1.4225,44.0,46.0
28,1.2295,48.0,48.0
29,1.3882,50.0,42.0
30,1.2615,42.0,50.0
31,1.2818,46.0,48.0
32,1.3230,50.0,50.0
33,1.3906,54.0,48.0
34,1.5020,54.0,48.0
35,1.2029,54.0,54.0
36,1.6488,58.0,44.0
37,1.3123,58.0,48.0
38,1.2104,54.0,46.0
39,1.6507,58.0,52.0
40,1.5349,56.0,48.0
41,1.4353,56.0,48.0
42,1.1842,60.0,52.0
43,1.3769,60.0,54.0
44,1.1259,56.0,52.0
45,1.1588,56.0,60.0
46,1.0577,60.0,52.0
47,1.1915,60.0,60.0
48,1.1539,60.0,58.0
49,1.1511,60.0,54.0
50,0.9813,64.0,62.0
51,0.9962,62.0,58.0
52,1.0499,60.0,52.0
53,1.3400,60.0,58.0
54,1.0190,62.0,62.0
55,1.1209,60.0,60.0
56,1.1350,62.0,58.0
57,0.9055,62.0,64.0
58,1.1406,66.0,62.0
59,0.9262,64.0,58.0
60,1.0897,64.0,64.0
61,1.2875,66.0,62.0
62,1.0752,62.0,64.0
63,1.0797,60.0,58.0
64,0.8752,66.0,64.0
65,0.9749,66.0,64.0
66,0.9185,66.0,60.0
67,0.9817,68.0,60.0
68,1.0042,64.0,62.0
69,0.8735,60.0,62.0
70,0.9882,60.0,58.0
71,1.2496,64.0,62.0
72,1.2871,66.0,58.0
73,1.1991,60.0,62.0
74,0.7931,66.0,56.0
75,0.8880,64.0,64.0
76,0.8069,68.0,58.0
77,0.7450,60.0,62.0
78,1.0823,68.0,62.0
79,0.7805,64.0,64.0
80,0.7102,60.0,58.0
81,0.7775,66.0,66.0
82,0.8307,66.0,64.0
83,1.2819,64.0,66.0
84,1.1330,68.0,64.0
85,0.7768,62.0,58.0
86,0.8481,66.0,60.0
87,1.0373,70.0,62.0
88,0.7821,62.0,60.0
89,0.9006,66.0,62.0
90,0.8919,64.0,62.0
91,0.7798,66.0,60.0
92,0.7650,66.0,64.0
93,0.8367,64.0,58.0
94,0.5635,64.0,62.0
95,0.9067,64.0,60.0
96,0.8314,62.0,60.0
97,0.8999,64.0,62.0
98,0.8389,66.0,62.0
99,1.0233,70.0,62.0
100,0.7628,68.0,66.0
