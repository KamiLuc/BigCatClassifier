batch_size: 64

learning_rate: 0.01

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3049,16.0,22.0
2,2.2655,16.0,20.0
3,2.2021,20.0,30.0
4,1.9768,16.0,16.0
5,1.8475,26.0,32.0
6,1.5502,22.0,34.0
7,1.6750,32.0,40.0
8,1.4907,32.0,40.0
9,1.5179,32.0,40.0
10,1.2942,48.0,54.0
11,1.5685,24.0,44.0
12,1.2984,38.0,56.0
13,1.4418,34.0,48.0
14,1.4303,42.0,48.0
15,1.0328,50.0,46.0
16,1.1989,50.0,58.0
17,1.2056,62.0,58.0
18,1.1263,62.0,56.0
19,0.7853,52.0,58.0
20,1.0434,46.0,44.0
21,1.0049,62.0,54.0
22,1.1550,66.0,56.0
23,0.5489,66.0,52.0
24,0.6927,62.0,54.0
25,0.6336,72.0,54.0
26,0.8686,76.0,56.0
27,0.9034,76.0,56.0
28,0.6608,70.0,54.0
29,0.8059,68.0,56.0
30,0.5956,66.0,60.0
31,1.0242,68.0,46.0
32,0.7771,68.0,60.0
33,0.7188,64.0,56.0
34,0.6507,76.0,62.0
35,0.4837,76.0,66.0
36,0.5352,80.0,56.0
37,0.3144,80.0,62.0
38,0.6971,70.0,56.0
39,0.4620,66.0,60.0
40,0.3091,74.0,66.0
41,0.1994,76.0,54.0
42,0.4605,78.0,62.0
43,0.3807,70.0,62.0
44,0.2939,76.0,60.0
45,0.1853,74.0,54.0
46,0.3164,74.0,68.0
47,0.3106,76.0,64.0
48,0.2602,72.0,64.0
49,0.5376,72.0,64.0
50,0.2618,72.0,72.0
51,0.0973,70.0,66.0
52,0.1794,78.0,62.0
53,0.1919,72.0,68.0
54,0.3026,74.0,64.0
55,0.1429,80.0,70.0
56,0.5383,76.0,58.0
57,0.2496,70.0,64.0
58,0.1402,76.0,70.0
59,0.2901,78.0,70.0
60,0.0533,74.0,62.0
61,0.1853,80.0,70.0
62,0.1917,78.0,68.0
63,0.2212,80.0,68.0
64,0.1908,82.0,66.0
65,0.1873,76.0,66.0
66,0.1680,78.0,66.0
67,0.1124,78.0,70.0
68,0.1409,82.0,64.0
69,0.0784,82.0,72.0
70,0.1198,82.0,76.0
71,0.0562,80.0,66.0
72,0.0792,78.0,68.0
73,0.0517,72.0,66.0
74,0.0193,78.0,68.0
75,0.0610,74.0,68.0
76,0.0790,80.0,70.0
77,0.3261,78.0,72.0
78,0.0297,82.0,68.0
79,0.0261,78.0,68.0
80,0.0227,74.0,70.0
81,0.1200,80.0,72.0
82,0.0370,76.0,68.0
83,0.0406,74.0,66.0
84,0.0048,74.0,70.0
85,0.0268,82.0,72.0
86,0.0128,80.0,68.0
87,0.0534,80.0,66.0
88,0.1216,80.0,70.0
89,0.1634,78.0,66.0
90,0.0836,72.0,68.0
91,0.0723,76.0,62.0
92,0.0775,76.0,68.0
93,0.1298,74.0,70.0
94,0.0374,76.0,70.0
95,0.2478,74.0,68.0
96,0.0849,76.0,70.0
97,0.0564,76.0,66.0
98,0.0838,76.0,70.0
99,0.0119,72.0,74.0
100,0.0292,78.0,70.0
