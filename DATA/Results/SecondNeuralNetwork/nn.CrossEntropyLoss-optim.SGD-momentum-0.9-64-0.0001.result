batch_size: 64

learning_rate: 0.0001

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3005,10.0,10.0
2,2.3064,10.0,10.0
3,2.2996,10.0,10.0
4,2.2975,10.0,10.0
5,2.2939,10.0,10.0
6,2.3003,10.0,10.0
7,2.3036,10.0,10.0
8,2.2957,14.0,8.0
9,2.2987,14.0,8.0
10,2.2986,8.0,10.0
11,2.3101,8.0,8.0
12,2.3013,8.0,8.0
13,2.2984,8.0,10.0
14,2.3087,10.0,10.0
15,2.3036,8.0,10.0
16,2.3002,10.0,10.0
17,2.2929,8.0,12.0
18,2.2956,10.0,14.0
19,2.3012,18.0,24.0
20,2.3007,14.0,20.0
21,2.2894,16.0,22.0
22,2.2962,18.0,28.0
23,2.3027,18.0,28.0
24,2.3035,18.0,28.0
25,2.3038,22.0,28.0
26,2.2933,24.0,30.0
27,2.3040,20.0,28.0
28,2.2892,24.0,26.0
29,2.2986,22.0,30.0
30,2.3051,16.0,26.0
31,2.2793,20.0,28.0
32,2.2859,22.0,30.0
33,2.2940,22.0,28.0
34,2.2915,22.0,28.0
35,2.3023,22.0,28.0
36,2.2927,24.0,28.0
37,2.2839,22.0,28.0
38,2.2745,22.0,28.0
39,2.2888,22.0,28.0
40,2.3069,20.0,28.0
41,2.2893,26.0,28.0
42,2.2897,26.0,30.0
43,2.2796,26.0,30.0
44,2.2986,26.0,28.0
45,2.2858,24.0,30.0
46,2.2869,24.0,28.0
47,2.2748,22.0,28.0
48,2.2761,20.0,28.0
49,2.2832,24.0,28.0
50,2.2796,22.0,28.0
51,2.2806,26.0,28.0
52,2.2742,26.0,28.0
53,2.2782,26.0,32.0
54,2.2864,24.0,32.0
55,2.2717,24.0,24.0
56,2.2773,24.0,28.0
57,2.2786,22.0,28.0
58,2.2965,24.0,28.0
59,2.2807,22.0,28.0
60,2.2655,20.0,28.0
61,2.2731,20.0,28.0
62,2.2703,22.0,28.0
63,2.2877,18.0,28.0
64,2.2909,26.0,30.0
65,2.2864,24.0,30.0
66,2.2703,26.0,28.0
67,2.2911,22.0,28.0
68,2.2590,22.0,28.0
69,2.2583,20.0,28.0
70,2.2625,22.0,30.0
71,2.2667,24.0,28.0
72,2.2726,24.0,28.0
73,2.2735,18.0,28.0
74,2.2702,18.0,30.0
75,2.2489,18.0,30.0
76,2.2646,20.0,28.0
77,2.2344,18.0,28.0
78,2.2421,22.0,30.0
79,2.2364,24.0,30.0
80,2.2516,20.0,30.0
81,2.2652,20.0,30.0
82,2.2124,24.0,28.0
83,2.2387,22.0,30.0
84,2.2499,20.0,30.0
85,2.2415,22.0,30.0
86,2.2512,20.0,32.0
87,2.2533,18.0,30.0
88,2.2147,22.0,32.0
89,2.2458,20.0,30.0
90,2.2032,18.0,30.0
91,2.2322,22.0,32.0
92,2.2028,22.0,30.0
93,2.2040,20.0,30.0
94,2.1824,24.0,28.0
95,2.2313,22.0,30.0
96,2.2497,24.0,28.0
97,2.2359,22.0,28.0
98,2.2742,22.0,28.0
99,2.2455,20.0,28.0
100,2.2037,20.0,30.0
