batch_size: 256

learning_rate: 0.0001

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2786,18.0,24.0
2,2.2725,14.0,20.0
3,2.2483,22.0,30.0
4,2.1871,20.0,30.0
5,2.0599,26.0,36.0
6,2.0686,20.0,34.0
7,1.9138,20.0,34.0
8,1.9559,24.0,36.0
9,1.8706,24.0,36.0
10,1.9884,24.0,40.0
11,1.9471,24.0,42.0
12,1.9166,24.0,42.0
13,1.8424,22.0,38.0
14,1.6382,22.0,42.0
15,1.7123,26.0,40.0
16,1.6360,30.0,38.0
17,1.6206,24.0,42.0
18,1.5112,26.0,44.0
19,1.5515,24.0,36.0
20,1.5946,28.0,48.0
21,1.5080,32.0,42.0
22,1.4740,34.0,40.0
23,1.7030,28.0,44.0
24,1.5173,40.0,42.0
25,1.6447,36.0,44.0
26,1.2924,46.0,40.0
27,1.5319,34.0,48.0
28,1.7100,40.0,46.0
29,1.4933,34.0,44.0
30,1.4425,42.0,44.0
31,1.7425,30.0,44.0
32,1.3266,38.0,42.0
33,1.5349,40.0,46.0
34,1.6281,32.0,44.0
35,1.6697,42.0,48.0
36,1.3943,32.0,42.0
37,1.8810,38.0,44.0
38,1.5494,42.0,46.0
39,1.4450,42.0,46.0
40,1.4723,44.0,48.0
41,1.3327,36.0,44.0
42,1.2054,46.0,44.0
43,1.4776,38.0,44.0
44,1.5032,48.0,36.0
45,1.3251,48.0,46.0
46,1.6116,44.0,48.0
47,1.4316,48.0,44.0
48,1.7552,34.0,50.0
49,1.4338,46.0,46.0
50,1.3372,48.0,50.0
51,1.4162,40.0,48.0
52,1.3783,46.0,42.0
53,1.4141,50.0,52.0
54,1.2003,46.0,44.0
55,1.2737,52.0,46.0
56,1.3895,46.0,46.0
57,1.1605,46.0,48.0
58,1.0807,48.0,42.0
59,1.2086,40.0,52.0
60,1.4381,50.0,40.0
61,1.1449,44.0,48.0
62,1.2224,50.0,46.0
63,1.1366,52.0,56.0
64,1.1918,56.0,52.0
65,1.2681,56.0,54.0
66,1.0948,52.0,52.0
67,1.4419,48.0,50.0
68,1.2968,50.0,48.0
69,1.0674,54.0,54.0
70,1.2411,50.0,46.0
71,1.4569,50.0,50.0
72,1.4771,46.0,56.0
73,1.3005,54.0,50.0
74,1.1840,56.0,56.0
75,1.0953,56.0,56.0
76,1.2955,54.0,52.0
77,1.5125,60.0,52.0
78,1.1842,52.0,50.0
79,1.1120,60.0,54.0
80,0.9741,56.0,50.0
81,1.1460,54.0,52.0
82,1.2041,60.0,48.0
83,1.2105,60.0,50.0
84,1.0596,60.0,54.0
85,1.4125,64.0,50.0
86,1.0603,56.0,56.0
87,1.2081,62.0,52.0
88,1.3090,60.0,52.0
89,1.2847,60.0,52.0
90,1.1553,60.0,56.0
91,1.0810,60.0,54.0
92,1.0873,68.0,52.0
93,1.0574,54.0,54.0
94,1.1849,60.0,50.0
95,1.0525,62.0,50.0
96,1.1490,62.0,58.0
97,0.9728,68.0,56.0
98,1.3287,66.0,58.0
99,0.9696,68.0,54.0
100,0.9147,58.0,58.0
101,1.0829,68.0,54.0
102,1.2268,66.0,50.0
103,1.0241,66.0,54.0
104,1.0590,58.0,54.0
105,0.9798,68.0,54.0
106,1.2501,64.0,54.0
107,1.3017,64.0,54.0
108,1.1252,64.0,54.0
109,0.9033,66.0,60.0
110,1.1105,62.0,50.0
111,1.0923,64.0,52.0
112,0.9364,60.0,52.0
113,0.7333,62.0,56.0
114,1.2865,64.0,60.0
115,1.0508,66.0,56.0
116,0.8089,62.0,54.0
117,0.9998,64.0,56.0
118,0.8261,64.0,56.0
119,0.9106,66.0,62.0
120,0.9009,66.0,56.0
121,0.9266,62.0,56.0
122,0.8182,66.0,56.0
123,0.6258,66.0,54.0
124,1.0552,70.0,56.0
125,1.1308,64.0,54.0
126,0.9700,60.0,56.0
127,0.9521,68.0,56.0
128,0.7828,62.0,58.0
129,0.8524,66.0,54.0
130,1.0653,58.0,56.0
131,0.7575,62.0,62.0
132,0.9280,58.0,66.0
133,0.9091,64.0,58.0
134,0.7793,62.0,54.0
135,1.0084,58.0,56.0
136,0.7720,68.0,58.0
137,1.0930,58.0,60.0
138,1.1038,62.0,54.0
139,0.8022,66.0,58.0
140,0.8540,70.0,62.0
141,0.8686,64.0,54.0
142,0.7273,66.0,58.0
143,0.8243,62.0,58.0
144,1.0835,62.0,58.0
145,0.9394,64.0,60.0
146,1.0588,60.0,56.0
147,0.7266,70.0,60.0
148,0.7628,62.0,56.0
149,0.9086,72.0,56.0
150,0.7696,68.0,56.0
