batch_size: 32

learning_rate: 0.01

model: BigCatClassifier2(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=50176, out_features=256, bias=True)
  (relu4): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.1631,10.0,20.0
2,2.1488,20.0,30.0
3,1.8450,18.0,24.0
4,1.1910,28.0,30.0
5,2.0676,26.0,44.0
6,1.6953,28.0,42.0
7,1.3822,38.0,44.0
8,1.4158,24.0,34.0
9,0.5529,40.0,54.0
10,1.1523,50.0,50.0
11,1.1877,48.0,48.0
12,1.8741,56.0,62.0
13,0.9770,58.0,58.0
14,1.7579,64.0,56.0
15,1.0973,52.0,50.0
16,1.3124,52.0,48.0
17,1.0898,48.0,56.0
18,0.6052,72.0,52.0
19,1.2776,70.0,56.0
20,1.1856,68.0,54.0
21,1.9335,62.0,58.0
22,0.6362,36.0,44.0
23,0.1456,70.0,52.0
24,1.4139,74.0,56.0
25,0.9531,74.0,66.0
26,0.3964,78.0,54.0
27,0.5411,78.0,60.0
28,0.2353,86.0,60.0
29,0.8573,80.0,62.0
30,0.0664,76.0,64.0
31,0.0646,76.0,62.0
32,0.4342,82.0,58.0
33,0.6429,78.0,66.0
34,0.0933,82.0,70.0
35,0.9211,80.0,56.0
36,1.4679,78.0,64.0
37,0.0378,72.0,60.0
38,0.1781,80.0,66.0
39,0.0110,80.0,64.0
40,0.3106,74.0,62.0
41,0.9476,74.0,60.0
42,0.1410,78.0,66.0
43,0.0568,72.0,62.0
44,0.0015,76.0,64.0
45,0.1081,86.0,64.0
46,0.1380,76.0,64.0
47,0.1934,82.0,68.0
48,0.1684,82.0,64.0
49,0.0001,84.0,64.0
50,0.0112,80.0,68.0
51,0.5615,88.0,62.0
52,0.0058,70.0,64.0
53,0.0729,80.0,68.0
54,0.0248,74.0,60.0
55,0.0551,84.0,60.0
56,0.0035,86.0,66.0
57,0.0654,82.0,64.0
58,0.2479,80.0,62.0
59,0.0000,74.0,66.0
60,0.6797,86.0,64.0
61,0.0317,68.0,66.0
62,0.0176,80.0,62.0
63,0.0028,84.0,60.0
64,0.0240,74.0,64.0
65,0.2124,88.0,66.0
66,0.0058,74.0,70.0
67,0.0293,78.0,62.0
68,0.1657,80.0,70.0
69,0.0399,82.0,66.0
70,1.1756,72.0,64.0
71,0.0007,78.0,62.0
72,0.0009,78.0,64.0
73,0.0678,80.0,66.0
74,0.0009,76.0,60.0
75,0.0022,78.0,64.0
76,0.0012,84.0,66.0
77,0.0947,78.0,68.0
78,0.0001,74.0,58.0
79,1.2147,76.0,62.0
80,0.0895,78.0,62.0
81,0.0163,78.0,64.0
82,0.0462,84.0,58.0
83,0.0021,80.0,62.0
84,0.0013,84.0,66.0
85,0.0015,88.0,64.0
86,0.1438,78.0,64.0
87,0.0006,80.0,68.0
88,0.0357,80.0,62.0
89,0.0001,80.0,62.0
90,0.0009,82.0,64.0
91,0.6551,78.0,62.0
92,0.0007,80.0,62.0
93,0.0035,76.0,64.0
94,0.0005,80.0,64.0
95,0.8809,72.0,62.0
96,1.7046,78.0,60.0
97,0.1048,80.0,70.0
98,0.0038,84.0,64.0
99,0.8446,80.0,74.0
100,0.0059,84.0,68.0
