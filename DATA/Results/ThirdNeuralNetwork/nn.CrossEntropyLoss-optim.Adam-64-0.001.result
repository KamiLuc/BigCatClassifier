batch_size: 64

learning_rate: 0.001

model: BigCatClassifier3(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu4): ReLU()
  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=25088, out_features=256, bias=True)
  (relu5): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.1439,22.0,38.0
2,1.9214,20.0,26.0
3,1.9852,22.0,24.0
4,1.6063,26.0,48.0
5,1.6820,28.0,44.0
6,1.3836,44.0,52.0
7,1.6334,42.0,52.0
8,1.3135,62.0,62.0
9,1.4099,58.0,60.0
10,1.5277,52.0,66.0
11,1.1711,62.0,58.0
12,1.0895,62.0,54.0
13,1.2146,66.0,56.0
14,1.0664,62.0,62.0
15,0.9575,52.0,50.0
16,1.2169,58.0,64.0
17,0.9735,62.0,58.0
18,1.1784,60.0,64.0
19,0.7249,58.0,54.0
20,0.9967,66.0,64.0
21,1.1064,70.0,64.0
22,0.8844,74.0,58.0
23,0.7617,64.0,60.0
24,0.7479,66.0,58.0
25,1.2942,70.0,64.0
26,0.7291,72.0,62.0
27,0.7092,70.0,62.0
28,0.7530,74.0,56.0
29,0.6734,74.0,58.0
30,0.7912,66.0,60.0
31,0.7353,76.0,56.0
32,0.6457,72.0,64.0
33,0.6001,74.0,62.0
34,0.9135,80.0,60.0
35,0.8438,78.0,62.0
36,0.7296,80.0,62.0
37,0.6559,70.0,60.0
38,0.4413,76.0,66.0
39,1.0718,74.0,60.0
40,0.6686,74.0,66.0
41,1.0106,80.0,64.0
42,0.5262,76.0,58.0
43,0.5825,76.0,60.0
44,0.5846,78.0,54.0
45,0.3315,78.0,62.0
46,0.5597,78.0,60.0
47,0.3193,74.0,62.0
48,0.6558,76.0,60.0
49,0.5284,76.0,66.0
50,0.5192,78.0,60.0
51,0.6813,76.0,62.0
52,0.5426,74.0,64.0
53,0.6708,76.0,64.0
54,0.3097,78.0,64.0
55,0.4337,76.0,64.0
56,0.4075,78.0,68.0
57,0.4067,82.0,68.0
58,0.5324,78.0,62.0
59,0.2942,76.0,68.0
60,0.4008,78.0,62.0
61,0.3426,74.0,60.0
62,0.4669,72.0,62.0
63,0.4571,78.0,62.0
64,1.0451,76.0,64.0
65,0.3580,80.0,64.0
66,0.1697,74.0,62.0
67,0.7348,78.0,62.0
68,0.1885,76.0,62.0
69,0.1591,78.0,56.0
70,0.4383,74.0,62.0
71,0.2655,76.0,66.0
72,0.3453,78.0,58.0
73,0.3393,76.0,56.0
74,0.2446,80.0,66.0
75,0.2439,78.0,64.0
76,0.4362,76.0,62.0
77,0.1897,80.0,62.0
78,0.2342,78.0,70.0
79,0.4478,76.0,64.0
80,0.3301,80.0,66.0
81,0.1580,80.0,72.0
82,0.3011,78.0,62.0
83,0.3171,80.0,68.0
84,0.4586,82.0,62.0
85,0.1458,82.0,62.0
86,0.1924,80.0,66.0
87,0.3412,80.0,66.0
88,0.2611,82.0,66.0
89,0.1883,74.0,66.0
90,0.3626,82.0,64.0
91,0.1941,78.0,62.0
92,0.1945,80.0,62.0
93,0.3480,82.0,66.0
94,0.1336,82.0,60.0
95,0.1391,84.0,64.0
96,0.0796,82.0,66.0
97,0.2027,78.0,70.0
98,0.1676,80.0,66.0
99,0.2609,78.0,58.0
100,0.1806,76.0,60.0
101,0.1707,82.0,66.0
102,0.1484,80.0,60.0
103,0.1515,80.0,64.0
104,0.1524,82.0,66.0
105,0.3021,80.0,56.0
106,0.3559,80.0,62.0
107,0.1586,78.0,66.0
108,0.1409,80.0,64.0
109,0.3021,76.0,60.0
110,0.1090,78.0,60.0
111,0.1728,78.0,64.0
112,0.3603,76.0,66.0
113,0.2162,76.0,58.0
114,0.7851,78.0,64.0
115,0.1566,82.0,66.0
116,0.1166,72.0,64.0
117,0.0360,72.0,66.0
118,0.1025,80.0,60.0
119,0.0926,78.0,62.0
120,0.1361,78.0,60.0
