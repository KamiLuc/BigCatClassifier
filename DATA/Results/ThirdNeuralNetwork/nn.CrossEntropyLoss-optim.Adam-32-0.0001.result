batch_size: 32

learning_rate: 0.0001

model: BigCatClassifier3(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu4): ReLU()
  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=25088, out_features=256, bias=True)
  (relu5): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2964,14.0,12.0
2,2.0011,26.0,36.0
3,1.9452,22.0,42.0
4,1.5842,26.0,38.0
5,1.4567,22.0,42.0
6,2.5689,26.0,42.0
7,2.0259,30.0,42.0
8,1.3849,26.0,42.0
9,1.5980,32.0,50.0
10,1.8946,42.0,46.0
11,1.9384,42.0,46.0
12,1.4471,36.0,40.0
13,1.1199,34.0,42.0
14,1.0735,38.0,46.0
15,1.5532,34.0,50.0
16,0.8174,48.0,48.0
17,1.4952,34.0,46.0
18,1.9561,44.0,42.0
19,1.4382,36.0,42.0
20,3.0053,40.0,44.0
21,2.0825,46.0,48.0
22,1.3063,44.0,48.0
23,1.6811,50.0,52.0
24,1.9470,46.0,54.0
25,1.2579,50.0,54.0
26,2.2671,46.0,48.0
27,1.5026,60.0,52.0
28,1.3986,56.0,58.0
29,1.2601,52.0,50.0
30,2.4702,56.0,58.0
31,1.1434,58.0,56.0
32,1.6511,54.0,54.0
33,2.8191,58.0,56.0
34,3.0819,62.0,60.0
35,1.6587,54.0,56.0
36,1.6067,60.0,52.0
37,1.3757,64.0,62.0
38,0.8619,66.0,64.0
39,1.0262,66.0,60.0
40,2.5207,66.0,58.0
41,0.3578,62.0,60.0
42,0.7401,58.0,62.0
43,0.9998,56.0,56.0
44,1.4144,62.0,64.0
45,1.2101,64.0,56.0
46,1.7042,44.0,56.0
47,1.2022,60.0,64.0
48,1.2203,58.0,58.0
49,1.1872,60.0,66.0
50,0.2674,68.0,62.0
51,0.5939,62.0,60.0
52,1.0389,66.0,64.0
53,0.5731,66.0,62.0
54,0.8851,64.0,64.0
55,1.0937,64.0,64.0
56,1.9145,66.0,62.0
57,1.0433,62.0,62.0
58,0.6278,62.0,60.0
59,1.6605,66.0,62.0
60,0.7911,64.0,60.0
61,1.5134,66.0,58.0
62,0.9416,58.0,66.0
63,1.5704,66.0,62.0
64,0.4754,72.0,62.0
65,0.4266,64.0,66.0
66,0.3979,66.0,60.0
67,0.5150,64.0,60.0
68,0.5907,64.0,66.0
69,0.3960,66.0,62.0
70,0.9202,72.0,66.0
71,0.3028,62.0,58.0
72,0.9322,64.0,64.0
73,0.6785,70.0,62.0
74,0.5063,68.0,62.0
75,1.8492,58.0,66.0
76,1.8022,68.0,66.0
77,1.0116,68.0,56.0
78,1.2102,64.0,62.0
79,0.7760,68.0,64.0
80,1.1243,58.0,60.0
81,0.5387,72.0,64.0
82,0.7107,64.0,60.0
83,0.5433,68.0,66.0
84,0.6107,66.0,64.0
85,0.8159,62.0,66.0
86,1.2262,62.0,64.0
87,0.7194,70.0,60.0
88,0.1678,62.0,58.0
89,1.0692,68.0,60.0
90,0.6561,66.0,60.0
91,0.9698,64.0,64.0
92,1.8401,62.0,66.0
93,0.8732,66.0,64.0
94,0.7008,60.0,66.0
95,1.3900,60.0,62.0
96,0.3916,64.0,70.0
97,0.6074,64.0,66.0
98,1.9171,58.0,60.0
99,0.5038,68.0,64.0
100,1.1136,68.0,68.0
101,0.8109,62.0,66.0
102,0.7927,68.0,60.0
103,0.5622,66.0,64.0
104,0.4269,70.0,58.0
105,0.5151,66.0,66.0
106,1.5740,66.0,62.0
107,0.1645,72.0,62.0
108,0.1220,78.0,66.0
109,0.3258,68.0,64.0
110,0.1733,72.0,64.0
111,0.2123,72.0,70.0
112,0.4208,70.0,64.0
113,0.7776,74.0,68.0
114,0.2642,70.0,66.0
115,0.3348,72.0,66.0
116,0.5198,76.0,70.0
117,0.2951,74.0,64.0
118,0.2464,70.0,64.0
119,1.0885,72.0,72.0
120,0.2853,68.0,66.0
