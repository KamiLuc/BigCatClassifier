batch_size: 32

learning_rate: 0.001

model: BigCatClassifier3(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu3): ReLU()
  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu4): ReLU()
  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=25088, out_features=256, bias=True)
  (relu5): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.5539,20.0,38.0
2,1.7003,20.0,32.0
3,1.8229,36.0,30.0
4,1.7215,34.0,32.0
5,1.0920,42.0,46.0
6,1.3996,46.0,54.0
7,1.6761,52.0,52.0
8,1.0483,56.0,50.0
9,0.6638,54.0,54.0
10,1.1254,52.0,54.0
11,1.6781,58.0,50.0
12,1.1989,50.0,42.0
13,1.3905,64.0,62.0
14,0.6230,60.0,62.0
15,0.3540,56.0,56.0
16,0.8861,60.0,62.0
17,1.2605,66.0,60.0
18,0.3927,54.0,50.0
19,1.2244,58.0,62.0
20,1.4749,66.0,60.0
21,0.8300,64.0,64.0
22,0.0858,66.0,58.0
23,0.4203,72.0,60.0
24,0.8692,66.0,58.0
25,0.8046,70.0,58.0
26,0.1025,66.0,56.0
27,1.0210,70.0,56.0
28,0.3148,74.0,62.0
29,0.4579,60.0,60.0
30,0.4520,64.0,64.0
31,0.3967,76.0,60.0
32,0.2103,76.0,54.0
33,0.7945,74.0,68.0
34,0.9452,74.0,64.0
35,0.2751,78.0,56.0
36,0.1111,78.0,62.0
37,0.5815,78.0,62.0
38,0.9029,74.0,54.0
39,0.6596,76.0,58.0
40,0.1217,76.0,70.0
41,0.1895,66.0,52.0
42,0.3847,74.0,70.0
43,0.2323,78.0,54.0
44,0.2448,68.0,60.0
45,0.0488,72.0,58.0
46,1.5872,74.0,66.0
47,0.0153,72.0,60.0
48,0.6213,72.0,66.0
49,0.1051,76.0,66.0
50,0.2212,74.0,60.0
51,0.0002,84.0,62.0
52,0.0701,82.0,64.0
53,0.0310,80.0,58.0
54,0.9859,76.0,54.0
55,0.5359,78.0,60.0
56,0.2979,78.0,60.0
57,0.5032,88.0,58.0
58,0.0186,82.0,60.0
59,0.0260,80.0,66.0
60,0.7795,78.0,62.0
61,0.0235,74.0,62.0
62,0.0025,76.0,66.0
63,0.0079,76.0,64.0
64,0.1191,82.0,62.0
65,0.3967,78.0,60.0
66,0.3806,78.0,66.0
67,0.2249,82.0,64.0
68,0.0081,84.0,60.0
69,0.0091,76.0,62.0
70,0.0024,76.0,68.0
71,0.1421,82.0,68.0
72,0.5620,74.0,62.0
73,0.0085,80.0,68.0
74,0.0284,78.0,70.0
75,0.0528,80.0,64.0
76,0.2998,76.0,58.0
77,0.0179,76.0,56.0
78,0.0071,76.0,68.0
79,0.0837,84.0,60.0
80,0.0056,78.0,62.0
81,0.0990,84.0,62.0
82,0.0120,80.0,62.0
83,0.2968,76.0,60.0
84,0.2817,74.0,60.0
85,0.2024,82.0,64.0
86,0.0269,80.0,70.0
87,0.0139,82.0,68.0
88,1.1653,74.0,56.0
89,0.0450,80.0,62.0
90,0.0695,84.0,66.0
91,0.0072,78.0,58.0
92,0.1391,80.0,62.0
93,0.0003,78.0,62.0
94,0.2995,80.0,60.0
95,0.6393,70.0,66.0
96,0.0766,82.0,66.0
97,0.0104,80.0,64.0
98,0.0713,74.0,70.0
99,0.0006,74.0,60.0
100,0.0016,78.0,62.0
101,0.0134,76.0,58.0
102,0.0072,78.0,58.0
103,0.2677,74.0,60.0
104,0.0322,80.0,62.0
105,0.0002,72.0,70.0
106,0.0346,82.0,62.0
107,0.0413,80.0,66.0
108,0.7274,84.0,68.0
109,0.9329,84.0,62.0
110,0.0021,78.0,60.0
111,0.0900,76.0,72.0
112,0.0000,80.0,68.0
113,0.0003,82.0,64.0
114,0.0241,78.0,60.0
115,0.0005,72.0,60.0
116,0.0000,80.0,62.0
117,0.0594,80.0,62.0
118,0.0004,80.0,62.0
119,0.0287,82.0,68.0
120,0.0621,76.0,68.0
