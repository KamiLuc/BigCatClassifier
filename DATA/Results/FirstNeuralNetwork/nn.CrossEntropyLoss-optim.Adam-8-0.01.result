batch_size: 8

learning_rate: 0.01

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2695,10.0,10.0
2,2.2658,10.0,10.0
3,2.3394,10.0,10.0
4,2.3120,10.0,10.0
5,2.3173,10.0,10.0
6,2.3164,10.0,10.0
7,2.3329,10.0,10.0
8,2.2665,10.0,10.0
9,2.2741,10.0,10.0
10,2.3117,10.0,10.0
11,2.2920,10.0,10.0
12,2.2997,10.0,10.0
13,2.3540,10.0,10.0
14,2.3374,10.0,10.0
15,2.2950,10.0,10.0
16,2.2787,10.0,10.0
17,2.3515,10.0,10.0
18,2.2991,10.0,10.0
19,2.3242,10.0,10.0
20,2.2957,10.0,10.0
21,2.3038,10.0,10.0
22,2.3052,10.0,10.0
23,2.2833,10.0,10.0
24,2.3102,10.0,10.0
25,2.2896,10.0,10.0
26,2.2724,10.0,10.0
27,2.2882,10.0,10.0
28,2.3317,10.0,10.0
29,2.3147,10.0,10.0
30,2.3415,10.0,10.0
31,2.2988,10.0,10.0
32,2.3381,10.0,10.0
33,2.3243,10.0,10.0
34,2.2502,10.0,10.0
35,2.2713,10.0,10.0
36,2.3413,10.0,10.0
37,2.3197,10.0,10.0
38,2.3048,10.0,10.0
39,2.2601,10.0,10.0
40,2.2879,10.0,10.0
41,2.3247,10.0,10.0
42,2.2796,10.0,10.0
43,2.2860,10.0,10.0
44,2.3587,10.0,10.0
45,2.3203,10.0,10.0
46,2.2904,10.0,10.0
47,2.2981,10.0,10.0
48,2.3060,10.0,10.0
49,2.3407,10.0,10.0
50,2.3375,10.0,10.0
51,2.2934,10.0,10.0
52,2.2955,10.0,10.0
53,2.3451,10.0,10.0
54,2.2916,10.0,10.0
55,2.3242,10.0,10.0
56,2.3052,10.0,10.0
57,2.3136,10.0,10.0
58,2.3659,10.0,10.0
59,2.3310,10.0,10.0
60,2.3656,10.0,10.0
61,2.2980,10.0,10.0
62,2.2393,10.0,10.0
63,2.3056,10.0,10.0
64,2.2859,10.0,10.0
65,2.3091,10.0,10.0
66,2.3759,10.0,10.0
67,2.3314,10.0,10.0
68,2.2916,10.0,10.0
69,2.2837,10.0,10.0
70,2.3147,10.0,10.0
71,2.2838,10.0,10.0
72,2.3517,10.0,10.0
73,2.3032,10.0,10.0
74,2.3365,10.0,10.0
75,2.3203,10.0,10.0
76,2.2966,10.0,10.0
77,2.2900,10.0,10.0
78,2.3073,10.0,10.0
79,2.2840,10.0,10.0
80,2.3178,10.0,10.0
81,2.2979,10.0,10.0
82,2.3084,10.0,10.0
83,2.3279,10.0,10.0
84,2.2879,10.0,10.0
85,2.3280,10.0,10.0
86,2.3453,10.0,10.0
87,2.3070,10.0,10.0
88,2.3098,10.0,10.0
89,2.3312,10.0,10.0
90,2.3132,10.0,10.0
91,2.3164,10.0,10.0
92,2.3097,10.0,10.0
93,2.3359,10.0,10.0
94,2.3278,10.0,10.0
95,2.2877,10.0,10.0
96,2.3199,10.0,10.0
97,2.2830,10.0,10.0
98,2.2849,10.0,10.0
99,2.3280,10.0,10.0
100,2.3127,10.0,10.0
101,2.2648,10.0,10.0
102,2.2895,10.0,10.0
103,2.3378,10.0,10.0
104,2.3306,10.0,10.0
105,2.3044,10.0,10.0
106,2.3165,10.0,10.0
107,2.3298,10.0,10.0
108,2.3795,10.0,10.0
109,2.3288,10.0,10.0
110,2.3265,10.0,10.0
111,2.2891,10.0,10.0
112,2.3093,10.0,10.0
113,2.2917,10.0,10.0
114,2.3120,10.0,10.0
115,2.3176,10.0,10.0
116,2.3203,10.0,10.0
117,2.3200,10.0,10.0
118,2.3217,10.0,10.0
119,2.2771,10.0,10.0
120,2.2653,10.0,10.0
121,2.2876,10.0,10.0
122,2.3004,10.0,10.0
123,2.3312,10.0,10.0
124,2.3124,10.0,10.0
125,2.3165,10.0,10.0
126,2.3226,10.0,10.0
127,2.3304,10.0,10.0
128,2.3145,10.0,10.0
129,2.3049,10.0,10.0
130,2.3100,10.0,10.0
131,2.3070,10.0,10.0
132,2.3148,10.0,10.0
133,2.3232,10.0,10.0
134,2.3478,10.0,10.0
135,2.3059,10.0,10.0
136,2.3397,10.0,10.0
137,2.3213,10.0,10.0
138,2.3085,10.0,10.0
139,2.2946,10.0,10.0
140,2.2899,10.0,10.0
141,2.3063,10.0,10.0
142,2.3610,10.0,10.0
143,2.2948,10.0,10.0
144,2.2823,10.0,10.0
145,2.3020,10.0,10.0
146,2.3059,10.0,10.0
147,2.3118,10.0,10.0
148,2.2573,10.0,10.0
149,2.2586,10.0,10.0
150,2.3128,10.0,10.0
151,2.3114,10.0,10.0
152,2.3283,10.0,10.0
153,2.3259,10.0,10.0
154,2.2862,10.0,10.0
155,2.2657,10.0,10.0
156,2.3276,10.0,10.0
157,2.2871,10.0,10.0
158,2.3033,10.0,10.0
159,2.2867,10.0,10.0
160,2.3311,10.0,10.0
161,2.2862,10.0,10.0
162,2.2836,10.0,10.0
163,2.3021,10.0,10.0
164,2.3438,10.0,10.0
165,2.3291,10.0,10.0
166,2.2656,10.0,10.0
167,2.2978,10.0,10.0
168,2.2627,10.0,10.0
169,2.3132,10.0,10.0
170,2.2936,10.0,10.0
171,2.3335,10.0,10.0
172,2.3127,10.0,10.0
173,2.3286,10.0,10.0
174,2.3169,10.0,10.0
175,2.2722,10.0,10.0
176,2.2782,10.0,10.0
177,2.2935,10.0,10.0
178,2.2882,10.0,10.0
179,2.3812,10.0,10.0
180,2.2801,10.0,10.0
181,2.3117,10.0,10.0
182,2.2550,10.0,10.0
183,2.3349,10.0,10.0
184,2.2960,10.0,10.0
185,2.3153,10.0,10.0
186,2.3580,10.0,10.0
187,2.3139,10.0,10.0
188,2.2612,10.0,10.0
189,2.2975,10.0,10.0
190,2.2818,10.0,10.0
191,2.2899,10.0,10.0
192,2.2958,10.0,10.0
193,2.2725,10.0,10.0
194,2.2722,10.0,10.0
195,2.3174,10.0,10.0
196,2.3077,10.0,10.0
197,2.3170,10.0,10.0
198,2.2792,10.0,10.0
199,2.3201,10.0,10.0
200,2.3191,10.0,10.0
