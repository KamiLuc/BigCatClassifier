batch_size: 32

learning_rate: 0.001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2906,18.0,20.0
2,2.2959,18.0,26.0
3,2.3490,18.0,28.0
4,2.2242,22.0,26.0
5,2.2883,22.0,20.0
6,2.1198,18.0,24.0
7,1.9365,22.0,22.0
8,1.6665,32.0,24.0
9,1.5857,34.0,26.0
10,1.7937,30.0,22.0
11,1.3572,40.0,38.0
12,1.7632,42.0,40.0
13,0.6574,32.0,36.0
14,1.5583,26.0,38.0
15,1.1970,22.0,38.0
16,1.7603,34.0,46.0
17,1.9041,36.0,44.0
18,1.8145,24.0,38.0
19,1.8345,26.0,36.0
20,1.5520,32.0,34.0
21,1.6008,40.0,42.0
22,1.5863,36.0,40.0
23,0.8724,52.0,52.0
24,1.3989,54.0,44.0
25,1.7110,56.0,48.0
26,1.1040,50.0,44.0
27,1.2989,50.0,48.0
28,0.9329,50.0,44.0
29,1.6691,50.0,46.0
30,1.4805,54.0,46.0
31,0.7693,40.0,50.0
32,1.3496,46.0,58.0
33,1.5024,50.0,46.0
34,1.7858,44.0,40.0
35,0.9982,46.0,50.0
36,0.7659,38.0,56.0
37,1.3772,56.0,42.0
38,0.7880,46.0,42.0
39,0.8892,56.0,48.0
40,0.9829,48.0,48.0
41,1.3527,50.0,52.0
42,1.7230,46.0,52.0
43,0.5582,50.0,48.0
44,1.1601,58.0,46.0
45,1.5112,48.0,44.0
46,0.6723,54.0,50.0
47,0.1616,60.0,42.0
48,1.2798,60.0,54.0
49,1.5722,60.0,56.0
50,0.2555,58.0,52.0
51,0.1777,54.0,50.0
52,1.1307,60.0,48.0
53,2.3480,62.0,58.0
54,0.7959,68.0,52.0
55,0.8483,54.0,46.0
56,1.2345,60.0,46.0
57,0.6610,54.0,46.0
58,1.3921,58.0,44.0
59,0.6119,64.0,48.0
60,0.6465,58.0,52.0
61,0.8800,60.0,48.0
62,0.6596,58.0,46.0
63,2.5038,52.0,52.0
64,0.9200,58.0,46.0
65,0.1020,62.0,52.0
66,0.4117,56.0,42.0
67,0.9141,60.0,48.0
68,0.9120,56.0,50.0
69,0.2171,66.0,54.0
70,0.0585,64.0,50.0
71,1.1450,62.0,48.0
72,0.0712,62.0,44.0
73,0.6962,64.0,48.0
74,0.0564,58.0,44.0
75,0.2904,60.0,48.0
76,0.1849,60.0,48.0
77,1.2034,62.0,46.0
78,0.5346,56.0,44.0
79,0.7404,64.0,46.0
80,0.1150,60.0,52.0
81,0.1283,66.0,48.0
82,1.2515,64.0,52.0
83,0.0529,62.0,54.0
84,0.7399,66.0,52.0
85,0.5001,66.0,46.0
86,0.8703,62.0,50.0
87,1.5535,64.0,50.0
88,1.0298,58.0,50.0
89,0.2803,70.0,58.0
90,0.7946,62.0,52.0
91,0.6380,54.0,52.0
92,0.0868,56.0,52.0
93,0.0079,54.0,46.0
94,0.0676,64.0,54.0
95,0.3458,64.0,50.0
96,1.1865,62.0,46.0
97,0.3165,64.0,54.0
98,0.0379,64.0,52.0
99,0.9833,62.0,56.0
100,0.2287,70.0,54.0
