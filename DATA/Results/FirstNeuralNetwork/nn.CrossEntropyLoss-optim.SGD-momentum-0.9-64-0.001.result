batch_size: 64

learning_rate: 0.001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3184,10.0,10.0
2,2.3190,14.0,20.0
3,2.2794,14.0,18.0
4,2.2281,20.0,28.0
5,2.2087,18.0,28.0
6,2.1779,24.0,34.0
7,2.1418,22.0,28.0
8,1.9296,22.0,30.0
9,2.0616,30.0,36.0
10,1.8967,36.0,40.0
11,1.8976,30.0,34.0
12,1.8647,30.0,42.0
13,1.8195,30.0,40.0
14,1.7475,28.0,40.0
15,1.7753,26.0,34.0
16,1.5847,32.0,38.0
17,1.7120,24.0,40.0
18,1.4975,26.0,42.0
19,1.6276,34.0,38.0
20,1.5273,38.0,44.0
21,1.7295,38.0,44.0
22,1.4260,38.0,46.0
23,1.4384,40.0,44.0
24,1.4977,44.0,46.0
25,1.6077,30.0,46.0
26,1.2070,30.0,38.0
27,1.3004,44.0,42.0
28,1.4820,34.0,50.0
29,1.3519,46.0,50.0
30,1.3507,36.0,46.0
31,1.2050,42.0,38.0
32,1.2787,52.0,50.0
33,1.4479,46.0,46.0
34,1.2608,50.0,50.0
35,1.5458,48.0,46.0
36,1.5014,36.0,42.0
37,1.0767,56.0,54.0
38,1.2519,40.0,50.0
39,1.3020,50.0,50.0
40,1.1238,46.0,48.0
41,1.3781,48.0,48.0
42,1.1760,50.0,50.0
43,1.1576,58.0,56.0
44,1.1935,52.0,50.0
45,1.1269,56.0,52.0
46,1.2516,44.0,46.0
47,1.3452,52.0,44.0
48,1.2519,54.0,44.0
49,0.9959,60.0,50.0
50,1.3071,54.0,56.0
51,1.3452,60.0,44.0
52,0.7468,62.0,40.0
53,1.1995,56.0,48.0
54,0.9889,58.0,56.0
55,1.0042,56.0,54.0
56,0.8809,50.0,44.0
57,1.0450,54.0,50.0
58,1.1261,54.0,58.0
59,0.9057,56.0,44.0
60,1.0139,50.0,52.0
61,0.8751,46.0,56.0
62,0.8580,56.0,54.0
63,0.8905,50.0,42.0
64,0.8284,54.0,46.0
65,0.8810,52.0,60.0
66,1.0580,62.0,52.0
67,0.9913,60.0,62.0
68,0.9093,56.0,52.0
69,0.9360,62.0,54.0
70,0.6781,52.0,56.0
71,0.8741,54.0,54.0
72,0.5935,54.0,58.0
73,0.9387,58.0,60.0
74,0.8168,56.0,58.0
75,0.6287,56.0,54.0
76,0.6618,58.0,50.0
77,0.6545,56.0,60.0
78,0.7448,60.0,54.0
79,1.1877,58.0,60.0
80,0.4397,58.0,50.0
81,0.7945,60.0,58.0
82,0.5083,58.0,52.0
83,0.7011,62.0,58.0
84,0.7086,56.0,56.0
85,0.7516,64.0,62.0
86,0.7225,54.0,46.0
87,0.4595,62.0,52.0
88,0.7376,64.0,52.0
89,0.7503,66.0,58.0
90,0.4413,60.0,56.0
91,0.5317,60.0,58.0
92,0.5278,64.0,48.0
93,0.6061,62.0,54.0
94,0.5658,60.0,52.0
95,0.6044,66.0,52.0
96,0.7088,58.0,52.0
97,0.4588,60.0,44.0
98,0.4207,62.0,50.0
99,0.3607,60.0,58.0
100,0.3761,62.0,54.0
