batch_size: 64

learning_rate: 0.0001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2370,16.0,22.0
2,2.0144,26.0,28.0
3,1.8847,28.0,32.0
4,1.8994,24.0,34.0
5,1.8542,26.0,36.0
6,1.7148,18.0,44.0
7,1.7499,24.0,38.0
8,1.3778,28.0,40.0
9,1.6378,30.0,36.0
10,1.5578,30.0,42.0
11,1.5496,28.0,46.0
12,1.6079,22.0,40.0
13,1.5721,32.0,44.0
14,1.4871,28.0,46.0
15,1.7497,42.0,44.0
16,1.3137,34.0,44.0
17,1.3612,30.0,48.0
18,1.5089,30.0,44.0
19,1.5950,38.0,46.0
20,1.2735,34.0,44.0
21,1.4596,36.0,40.0
22,1.2788,38.0,42.0
23,1.3644,40.0,46.0
24,1.1989,36.0,44.0
25,1.4718,44.0,44.0
26,1.3739,42.0,44.0
27,1.3758,40.0,44.0
28,1.7172,44.0,50.0
29,1.3880,42.0,42.0
30,1.1327,50.0,50.0
31,1.3079,48.0,46.0
32,1.2842,48.0,48.0
33,1.2216,50.0,50.0
34,1.1126,52.0,46.0
35,1.3719,52.0,44.0
36,1.1607,54.0,46.0
37,1.2910,42.0,46.0
38,1.4722,54.0,50.0
39,1.1130,48.0,46.0
40,1.2389,50.0,50.0
41,1.0506,50.0,40.0
42,1.1987,54.0,46.0
43,1.1605,48.0,50.0
44,1.1764,52.0,48.0
45,1.4070,44.0,56.0
46,1.1596,48.0,48.0
47,1.1804,50.0,46.0
48,1.0674,54.0,52.0
49,1.2641,48.0,50.0
50,0.9902,52.0,52.0
51,0.9425,52.0,46.0
52,0.9347,52.0,50.0
53,0.9978,52.0,50.0
54,0.8491,48.0,54.0
55,1.0948,54.0,52.0
56,1.0561,48.0,52.0
57,0.9232,54.0,54.0
58,1.0550,48.0,48.0
59,0.9621,54.0,54.0
60,1.0268,60.0,50.0
61,0.8312,56.0,54.0
62,0.9312,48.0,52.0
63,0.9845,54.0,48.0
64,1.0015,54.0,52.0
65,1.1673,54.0,56.0
66,0.9557,58.0,60.0
67,0.8251,56.0,54.0
68,1.0204,54.0,56.0
69,1.0662,52.0,56.0
70,0.9912,56.0,50.0
71,0.9691,54.0,54.0
72,0.9930,58.0,50.0
73,0.8526,58.0,52.0
74,0.6634,56.0,50.0
75,0.6530,58.0,50.0
76,0.9253,54.0,52.0
77,0.7762,54.0,56.0
78,0.7011,56.0,58.0
79,0.9603,52.0,54.0
80,0.8259,54.0,52.0
81,0.8597,56.0,52.0
82,0.7331,52.0,58.0
83,0.8487,60.0,58.0
84,1.0197,54.0,54.0
85,0.8837,54.0,56.0
86,1.0184,52.0,58.0
87,0.9512,54.0,56.0
88,0.7525,54.0,52.0
89,0.8275,50.0,54.0
90,0.6379,58.0,64.0
91,0.8288,56.0,56.0
92,0.9692,56.0,64.0
93,0.6153,52.0,62.0
94,0.6748,52.0,58.0
95,0.6605,58.0,52.0
96,0.6893,56.0,56.0
97,0.9161,58.0,62.0
98,0.9179,56.0,60.0
99,0.8600,58.0,58.0
100,0.6898,60.0,58.0
