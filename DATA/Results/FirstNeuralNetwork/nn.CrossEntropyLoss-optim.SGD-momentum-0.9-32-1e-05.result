batch_size: 32

learning_rate: 1e-05

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3301,12.0,4.0
2,2.2947,10.0,6.0
3,2.3025,10.0,12.0
4,2.3392,10.0,12.0
5,2.3481,10.0,10.0
6,2.3053,14.0,10.0
7,2.3302,18.0,8.0
8,2.2895,14.0,10.0
9,2.2917,18.0,10.0
10,2.2613,18.0,8.0
11,2.2488,18.0,8.0
12,2.2822,18.0,16.0
13,2.2383,18.0,16.0
14,2.2847,16.0,18.0
15,2.3226,18.0,18.0
16,2.3286,18.0,16.0
17,2.2844,16.0,18.0
18,2.2824,18.0,22.0
19,2.2786,20.0,24.0
20,2.2455,20.0,24.0
21,2.2454,18.0,22.0
22,2.3124,20.0,24.0
23,2.2782,18.0,26.0
24,2.3051,12.0,26.0
25,2.3460,14.0,26.0
26,2.3439,14.0,26.0
27,2.2961,16.0,28.0
28,2.2981,16.0,26.0
29,2.2861,18.0,24.0
30,2.2729,18.0,24.0
31,2.3255,16.0,26.0
32,2.2234,16.0,26.0
33,2.2435,16.0,28.0
34,2.2509,20.0,24.0
35,2.2766,22.0,26.0
36,2.2854,24.0,28.0
37,2.2755,24.0,28.0
38,2.2184,24.0,28.0
39,2.2168,22.0,26.0
40,2.2817,20.0,26.0
41,2.2656,16.0,26.0
42,2.3286,16.0,26.0
43,2.2812,16.0,26.0
44,2.3443,20.0,26.0
45,2.2199,20.0,28.0
46,2.2621,20.0,26.0
47,2.2073,18.0,28.0
48,2.2534,16.0,26.0
49,2.3124,22.0,26.0
50,2.2993,24.0,28.0
51,2.2647,22.0,28.0
52,2.2441,22.0,26.0
53,2.2760,24.0,28.0
54,2.2378,20.0,26.0
55,2.3094,24.0,28.0
56,2.2209,24.0,26.0
57,2.2875,22.0,26.0
58,2.2249,24.0,26.0
59,2.2884,24.0,26.0
60,2.2642,22.0,26.0
61,2.3057,24.0,26.0
62,2.2510,20.0,28.0
63,2.2712,20.0,28.0
64,2.3566,18.0,28.0
65,2.3286,20.0,28.0
66,2.3999,20.0,28.0
67,2.2433,20.0,28.0
68,2.2542,22.0,26.0
69,2.2916,20.0,26.0
70,2.1863,22.0,28.0
71,2.2606,18.0,28.0
72,2.3120,20.0,28.0
73,2.1645,20.0,26.0
74,2.2219,20.0,28.0
75,2.3385,20.0,28.0
76,2.1811,22.0,28.0
77,2.2850,22.0,28.0
78,2.2847,20.0,30.0
79,2.1321,20.0,30.0
80,2.3214,20.0,28.0
81,2.1879,22.0,28.0
82,2.2668,22.0,28.0
83,2.1855,20.0,28.0
84,2.1586,20.0,28.0
85,2.1340,22.0,28.0
86,2.2794,20.0,28.0
87,2.2538,20.0,30.0
88,2.1864,22.0,28.0
89,2.2145,22.0,28.0
90,2.1672,20.0,30.0
91,2.1901,24.0,28.0
92,2.2741,26.0,30.0
93,2.1785,20.0,28.0
94,2.2326,22.0,32.0
95,2.2022,24.0,28.0
96,2.2527,22.0,28.0
97,2.2330,24.0,28.0
98,2.2510,22.0,28.0
99,2.3568,22.0,28.0
100,2.2354,22.0,28.0
