batch_size: 128

learning_rate: 0.001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2845,12.0,10.0
2,1.9926,26.0,36.0
3,1.8021,20.0,38.0
4,1.5020,24.0,36.0
5,1.4922,32.0,44.0
6,1.3960,34.0,42.0
7,1.7997,40.0,50.0
8,1.4385,48.0,44.0
9,1.1201,40.0,52.0
10,1.1012,56.0,48.0
11,1.2069,38.0,48.0
12,1.1223,56.0,58.0
13,1.1131,56.0,52.0
14,0.8777,46.0,50.0
15,0.9594,46.0,48.0
16,0.9254,48.0,40.0
17,0.7531,54.0,54.0
18,0.7455,56.0,58.0
19,0.6493,56.0,58.0
20,0.5532,60.0,54.0
21,0.7660,62.0,56.0
22,0.5652,58.0,54.0
23,0.6434,56.0,56.0
24,0.4403,62.0,48.0
25,0.6327,52.0,52.0
26,0.3675,44.0,52.0
27,0.6329,50.0,54.0
28,0.5960,58.0,54.0
29,0.4059,54.0,54.0
30,0.4246,62.0,58.0
31,0.4854,62.0,54.0
32,0.3461,52.0,50.0
33,0.2797,42.0,36.0
34,0.1664,52.0,44.0
35,0.2154,56.0,50.0
36,0.3227,58.0,54.0
37,0.2249,62.0,60.0
38,0.2293,54.0,54.0
39,0.1051,42.0,54.0
40,0.1097,54.0,56.0
41,0.0788,60.0,58.0
42,0.1772,52.0,54.0
43,0.3776,54.0,54.0
44,0.2007,62.0,56.0
45,0.1172,44.0,52.0
46,0.0837,54.0,54.0
47,0.1267,50.0,60.0
48,0.0526,48.0,50.0
49,0.0718,56.0,54.0
50,0.0537,52.0,50.0
51,0.0312,52.0,50.0
52,0.0246,48.0,56.0
53,0.1079,58.0,60.0
54,0.0350,58.0,50.0
55,0.0412,50.0,48.0
56,0.1108,48.0,46.0
57,0.0323,46.0,50.0
58,0.0248,52.0,56.0
59,0.0460,54.0,56.0
60,0.0263,48.0,54.0
61,0.0563,54.0,54.0
62,0.0561,48.0,50.0
63,0.0281,54.0,56.0
64,0.0372,54.0,46.0
65,0.0461,46.0,48.0
66,0.0106,54.0,56.0
67,0.0101,48.0,48.0
68,0.0335,52.0,60.0
69,0.0172,44.0,46.0
70,0.0266,48.0,50.0
71,0.0204,48.0,46.0
72,0.0308,48.0,46.0
73,0.0091,48.0,54.0
74,0.0536,48.0,56.0
75,0.0975,54.0,48.0
76,0.0205,46.0,50.0
77,0.0990,46.0,54.0
78,0.0568,52.0,44.0
79,0.0526,56.0,52.0
80,0.0057,52.0,46.0
81,0.0069,56.0,52.0
82,0.0489,52.0,52.0
83,0.0558,54.0,46.0
84,0.0578,56.0,58.0
85,0.0156,52.0,52.0
86,0.0064,54.0,54.0
87,0.0115,58.0,54.0
88,0.0087,52.0,54.0
89,0.0330,54.0,50.0
90,0.0032,44.0,50.0
91,0.0024,58.0,54.0
92,0.0214,52.0,56.0
93,0.0035,50.0,50.0
94,0.0106,48.0,50.0
95,0.0111,56.0,44.0
96,0.0031,54.0,60.0
97,0.0042,52.0,54.0
98,0.0065,54.0,48.0
99,0.0040,52.0,54.0
100,0.0028,58.0,54.0
