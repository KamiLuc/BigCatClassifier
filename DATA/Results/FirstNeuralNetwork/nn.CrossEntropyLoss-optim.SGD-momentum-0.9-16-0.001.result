batch_size: 16

learning_rate: 0.001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2960,8.0,18.0
2,2.1217,20.0,24.0
3,2.1880,14.0,28.0
4,1.6014,20.0,28.0
5,1.2983,28.0,42.0
6,0.7915,28.0,30.0
7,1.8309,26.0,40.0
8,2.0388,28.0,38.0
9,1.3950,36.0,48.0
10,0.7222,42.0,50.0
11,2.4405,36.0,30.0
12,1.1656,34.0,40.0
13,1.2597,24.0,38.0
14,0.9515,50.0,44.0
15,1.5043,64.0,48.0
16,0.7688,60.0,46.0
17,1.5677,50.0,52.0
18,0.9270,56.0,48.0
19,1.8388,60.0,44.0
20,1.3870,42.0,36.0
21,1.1551,44.0,54.0
22,0.3747,40.0,38.0
23,1.4381,56.0,50.0
24,1.6680,50.0,54.0
25,0.2683,48.0,50.0
26,0.5702,54.0,48.0
27,1.8227,56.0,58.0
28,0.5108,60.0,48.0
29,0.9465,50.0,52.0
30,0.7281,56.0,54.0
31,0.3158,70.0,50.0
32,1.0744,62.0,52.0
33,0.7637,66.0,54.0
34,0.7671,64.0,54.0
35,0.6431,66.0,56.0
36,0.8024,64.0,60.0
37,1.7586,60.0,54.0
38,0.2924,58.0,54.0
39,1.0206,64.0,58.0
40,0.6106,58.0,56.0
41,1.5366,64.0,52.0
42,0.1466,66.0,50.0
43,0.7629,64.0,60.0
44,0.1395,64.0,62.0
45,0.3720,70.0,64.0
46,1.5317,62.0,56.0
47,0.3907,50.0,52.0
48,1.1927,68.0,56.0
49,0.4675,66.0,56.0
50,0.4105,70.0,60.0
51,0.3097,60.0,56.0
52,0.6830,64.0,58.0
53,0.1780,58.0,50.0
54,0.1891,70.0,66.0
55,0.0120,64.0,56.0
56,0.8200,58.0,56.0
57,0.2961,64.0,58.0
58,0.1009,68.0,60.0
59,0.1052,60.0,60.0
60,0.7557,66.0,66.0
61,0.1507,62.0,60.0
62,0.4666,60.0,54.0
63,0.0540,64.0,58.0
64,0.1547,70.0,60.0
65,0.1903,66.0,58.0
66,0.0417,68.0,58.0
67,0.0009,60.0,56.0
68,0.5200,64.0,64.0
69,0.0401,62.0,58.0
70,0.0605,66.0,62.0
71,0.1092,72.0,64.0
72,0.0176,68.0,60.0
73,0.2082,68.0,66.0
74,0.1680,74.0,66.0
75,0.0926,70.0,60.0
76,0.3872,72.0,60.0
77,0.0014,66.0,58.0
78,0.2959,70.0,60.0
79,0.1967,68.0,58.0
80,0.0144,72.0,56.0
81,0.2406,74.0,58.0
82,0.1349,68.0,64.0
83,0.0286,72.0,64.0
84,0.5087,70.0,62.0
85,0.0057,72.0,60.0
86,0.0740,64.0,52.0
87,0.0081,68.0,54.0
88,0.0266,68.0,62.0
89,0.0166,74.0,66.0
90,0.0172,62.0,52.0
91,0.0139,68.0,64.0
92,0.0974,62.0,60.0
93,0.0051,68.0,62.0
94,0.4027,68.0,56.0
95,0.0044,60.0,54.0
96,0.0244,72.0,60.0
97,0.2847,68.0,66.0
98,0.5777,70.0,56.0
99,0.0241,66.0,58.0
100,0.0003,68.0,60.0
