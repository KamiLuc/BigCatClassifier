batch_size: 32

learning_rate: 0.0001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3454,28.0,42.0
2,2.2285,28.0,30.0
3,1.9122,30.0,44.0
4,1.4414,18.0,42.0
5,1.7531,18.0,34.0
6,1.6963,34.0,40.0
7,1.4353,28.0,36.0
8,2.3076,36.0,44.0
9,2.1700,34.0,48.0
10,2.4982,34.0,40.0
11,1.5329,28.0,44.0
12,1.4774,38.0,42.0
13,2.7534,32.0,48.0
14,1.6012,36.0,50.0
15,0.9944,36.0,46.0
16,0.9563,36.0,50.0
17,1.2138,46.0,44.0
18,1.7300,42.0,50.0
19,1.2229,46.0,44.0
20,1.7171,40.0,42.0
21,1.3141,50.0,48.0
22,1.3483,48.0,52.0
23,1.5458,42.0,46.0
24,1.4522,50.0,52.0
25,1.9495,44.0,52.0
26,1.1859,50.0,54.0
27,1.2697,52.0,56.0
28,0.9672,54.0,48.0
29,1.2961,52.0,52.0
30,0.9838,56.0,52.0
31,1.1805,52.0,46.0
32,2.6007,50.0,56.0
33,0.7628,56.0,52.0
34,0.8299,54.0,48.0
35,2.2416,50.0,46.0
36,2.5890,54.0,56.0
37,1.3378,52.0,56.0
38,0.8626,54.0,54.0
39,1.2254,54.0,54.0
40,0.9681,54.0,52.0
41,0.9973,54.0,52.0
42,0.9904,58.0,54.0
43,1.4037,54.0,60.0
44,1.3096,56.0,56.0
45,0.5972,58.0,46.0
46,1.3897,60.0,48.0
47,0.9491,60.0,54.0
48,0.7540,58.0,54.0
49,1.4107,62.0,54.0
50,0.8681,64.0,46.0
51,1.0125,60.0,50.0
52,0.6666,66.0,56.0
53,0.7615,64.0,56.0
54,0.8982,64.0,56.0
55,1.0700,62.0,50.0
56,0.8308,62.0,56.0
57,0.7629,58.0,48.0
58,0.6538,62.0,54.0
59,0.4712,66.0,52.0
60,1.6090,58.0,52.0
61,1.2146,66.0,56.0
62,1.6333,64.0,62.0
63,0.7463,64.0,50.0
64,0.5465,64.0,48.0
65,0.4593,60.0,58.0
66,1.4863,66.0,50.0
67,0.3234,62.0,54.0
68,0.5124,66.0,50.0
69,0.4919,62.0,58.0
70,0.5018,60.0,54.0
71,1.2085,66.0,56.0
72,1.0333,62.0,52.0
73,1.2753,58.0,56.0
74,1.0671,66.0,54.0
75,0.3098,56.0,58.0
76,1.9221,62.0,62.0
77,0.9134,64.0,52.0
78,0.5091,62.0,60.0
79,0.4841,68.0,60.0
80,1.4305,66.0,60.0
81,0.4527,62.0,54.0
82,1.7966,62.0,54.0
83,0.5339,54.0,58.0
84,0.2398,60.0,58.0
85,1.0092,68.0,58.0
86,0.5276,62.0,54.0
87,0.6402,66.0,54.0
88,0.4855,64.0,58.0
89,0.2629,64.0,62.0
90,0.1552,58.0,54.0
91,0.9777,66.0,60.0
92,1.2560,62.0,48.0
93,0.4292,62.0,52.0
94,0.8637,64.0,58.0
95,0.2760,68.0,58.0
96,0.8330,64.0,58.0
97,0.6104,68.0,50.0
98,0.1746,72.0,56.0
99,0.1029,62.0,52.0
100,0.5309,64.0,62.0
101,0.3663,64.0,60.0
102,0.2774,64.0,56.0
103,0.4335,66.0,62.0
104,1.3101,66.0,60.0
105,0.6184,66.0,58.0
106,0.4047,68.0,62.0
107,0.6026,72.0,54.0
108,0.8728,70.0,58.0
109,0.2991,70.0,60.0
110,0.2413,66.0,60.0
111,1.3349,68.0,62.0
112,0.0618,70.0,62.0
113,1.3624,70.0,62.0
114,0.4687,64.0,58.0
115,0.5510,64.0,62.0
116,0.1247,66.0,62.0
117,0.5155,66.0,64.0
118,0.8070,60.0,60.0
119,0.2997,64.0,60.0
120,0.3764,66.0,60.0
121,1.0687,62.0,58.0
122,0.2130,62.0,66.0
123,0.5555,68.0,58.0
124,0.1475,68.0,52.0
125,0.4867,66.0,54.0
126,0.5270,70.0,58.0
127,0.1133,64.0,50.0
128,1.0614,64.0,56.0
129,0.1739,60.0,58.0
130,1.8973,64.0,60.0
131,0.5955,72.0,62.0
132,0.0798,70.0,62.0
133,0.5425,60.0,60.0
134,0.4500,68.0,68.0
135,0.4182,74.0,64.0
136,0.3575,68.0,58.0
137,0.2393,68.0,64.0
138,0.3274,72.0,62.0
139,0.9934,70.0,62.0
140,0.6501,66.0,62.0
141,0.5267,58.0,62.0
142,0.1876,62.0,60.0
143,0.8922,72.0,62.0
144,0.0356,64.0,54.0
145,0.3614,70.0,56.0
146,0.7268,64.0,60.0
147,0.4759,66.0,64.0
148,0.7297,66.0,58.0
149,1.1674,66.0,62.0
150,0.1167,66.0,66.0
