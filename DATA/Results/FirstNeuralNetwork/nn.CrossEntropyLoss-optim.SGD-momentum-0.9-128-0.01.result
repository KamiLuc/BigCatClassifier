batch_size: 128

learning_rate: 0.01

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2885,14.0,12.0
2,2.2603,20.0,22.0
3,2.2265,14.0,22.0
4,2.1761,20.0,26.0
5,2.0002,26.0,30.0
6,1.8086,32.0,32.0
7,1.6548,30.0,32.0
8,1.6507,34.0,46.0
9,1.5528,28.0,36.0
10,1.4679,36.0,46.0
11,1.6119,32.0,42.0
12,1.6455,40.0,50.0
13,1.6275,40.0,44.0
14,1.8529,48.0,48.0
15,1.1969,46.0,48.0
16,1.3196,46.0,44.0
17,1.5012,52.0,46.0
18,1.1538,50.0,48.0
19,1.6445,46.0,42.0
20,1.3299,50.0,40.0
21,1.2683,48.0,54.0
22,1.0230,38.0,36.0
23,1.3432,46.0,52.0
24,1.0967,54.0,42.0
25,1.4559,44.0,42.0
26,1.0979,56.0,54.0
27,1.0848,60.0,50.0
28,0.8319,58.0,54.0
29,1.3199,62.0,48.0
30,1.1276,56.0,50.0
31,1.0983,62.0,52.0
32,0.8798,54.0,56.0
33,1.0544,62.0,56.0
34,0.7273,66.0,56.0
35,1.1363,62.0,52.0
36,1.1661,62.0,56.0
37,0.8424,54.0,56.0
38,0.7657,52.0,44.0
39,1.1796,68.0,56.0
40,0.6716,56.0,46.0
41,0.7320,62.0,54.0
42,0.9902,68.0,58.0
43,1.0902,66.0,52.0
44,0.5674,66.0,56.0
45,0.9443,64.0,54.0
46,0.4382,68.0,62.0
47,0.6328,62.0,62.0
48,0.7511,68.0,58.0
49,0.8431,70.0,60.0
50,0.4858,64.0,52.0
51,0.6776,64.0,48.0
52,0.5259,74.0,54.0
53,0.5444,68.0,50.0
54,0.5084,70.0,54.0
55,0.7370,72.0,62.0
56,0.5996,70.0,62.0
57,0.3726,74.0,62.0
58,0.3517,76.0,52.0
59,0.8016,74.0,54.0
60,0.5950,72.0,56.0
61,0.4782,74.0,58.0
62,0.2544,68.0,62.0
63,0.4931,72.0,60.0
64,0.3157,68.0,68.0
65,0.4016,64.0,58.0
66,0.3571,64.0,66.0
67,0.3564,70.0,58.0
68,0.1930,70.0,56.0
69,0.3168,66.0,64.0
70,0.1896,68.0,58.0
71,0.3404,66.0,56.0
72,0.4098,68.0,56.0
73,0.3062,60.0,54.0
74,0.2405,70.0,58.0
75,0.1882,62.0,58.0
76,0.3263,76.0,58.0
77,0.2278,72.0,62.0
78,0.2200,66.0,62.0
79,0.2743,72.0,56.0
80,0.4283,74.0,62.0
81,0.1773,72.0,66.0
82,0.1306,68.0,60.0
83,0.2513,64.0,50.0
84,0.0867,66.0,56.0
85,0.1586,66.0,58.0
86,0.3253,78.0,62.0
87,0.3449,72.0,66.0
88,0.0987,74.0,58.0
89,0.1970,70.0,54.0
90,0.1397,70.0,62.0
91,0.2271,80.0,64.0
92,0.2733,74.0,58.0
93,0.1887,76.0,58.0
94,0.0884,70.0,62.0
95,0.1961,68.0,54.0
96,0.3487,78.0,56.0
97,0.2302,70.0,64.0
98,0.1655,72.0,62.0
99,0.2947,74.0,60.0
100,0.1041,74.0,62.0
