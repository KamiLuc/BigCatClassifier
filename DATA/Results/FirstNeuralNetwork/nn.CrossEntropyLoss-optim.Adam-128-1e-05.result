batch_size: 128

learning_rate: 1e-05

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3197,14.0,16.0
2,2.2890,14.0,18.0
3,2.2573,16.0,24.0
4,2.2496,20.0,24.0
5,2.1475,16.0,24.0
6,2.1706,24.0,28.0
7,2.1292,22.0,26.0
8,2.0980,20.0,30.0
9,2.1378,28.0,30.0
10,2.0388,22.0,28.0
11,2.0707,34.0,34.0
12,2.0769,30.0,24.0
13,2.0494,20.0,36.0
14,1.9881,32.0,32.0
15,1.9218,26.0,32.0
16,1.8688,30.0,34.0
17,2.0309,24.0,36.0
18,1.8698,28.0,36.0
19,1.9159,34.0,32.0
20,1.8412,28.0,36.0
21,1.8804,28.0,36.0
22,1.8348,26.0,36.0
23,1.8694,32.0,38.0
24,1.9847,28.0,40.0
25,1.8500,20.0,38.0
26,1.8178,36.0,34.0
27,1.8565,32.0,38.0
28,1.6695,30.0,40.0
29,1.7735,32.0,38.0
30,1.5123,26.0,34.0
31,1.7058,24.0,40.0
32,1.6373,30.0,42.0
33,1.7800,22.0,42.0
34,1.5820,24.0,38.0
35,1.5052,38.0,34.0
36,1.6226,32.0,40.0
37,1.6822,26.0,44.0
38,1.5548,26.0,42.0
39,1.6425,36.0,40.0
40,1.7474,22.0,42.0
41,1.6369,30.0,36.0
42,1.6832,32.0,40.0
43,1.6863,26.0,42.0
44,1.5077,34.0,36.0
45,1.5118,28.0,44.0
46,1.6420,26.0,42.0
47,1.6885,32.0,42.0
48,1.6535,26.0,46.0
49,1.6084,30.0,44.0
50,1.7300,32.0,36.0
51,1.4707,26.0,40.0
52,1.5687,30.0,40.0
53,1.5333,30.0,44.0
54,1.4909,30.0,40.0
55,1.4993,28.0,38.0
56,1.5419,28.0,42.0
57,1.5280,30.0,38.0
58,1.4015,30.0,40.0
59,1.4388,30.0,42.0
60,1.4536,32.0,38.0
61,1.6087,28.0,44.0
62,1.2513,34.0,46.0
63,1.6861,28.0,46.0
64,1.4590,30.0,44.0
65,1.5511,36.0,42.0
66,1.5008,30.0,46.0
67,1.3313,32.0,42.0
68,1.4753,34.0,36.0
69,1.5310,30.0,48.0
70,1.5228,30.0,48.0
71,1.3128,28.0,44.0
72,1.2414,28.0,46.0
73,1.4956,28.0,44.0
74,1.6796,34.0,42.0
75,1.5101,34.0,48.0
76,1.7620,30.0,42.0
77,1.2641,36.0,46.0
78,1.5107,34.0,46.0
79,1.2862,30.0,46.0
80,1.6997,32.0,40.0
81,1.4307,34.0,42.0
82,1.2689,30.0,48.0
83,1.4504,32.0,44.0
84,1.4314,28.0,42.0
85,1.5668,30.0,44.0
86,1.4585,36.0,46.0
87,1.4813,34.0,48.0
88,1.3227,36.0,40.0
89,1.4815,36.0,46.0
90,1.5042,38.0,48.0
91,1.3733,34.0,44.0
92,1.4554,32.0,46.0
93,1.4017,32.0,44.0
94,1.2205,36.0,48.0
95,1.2033,34.0,46.0
96,1.5275,34.0,44.0
97,1.4327,38.0,50.0
98,1.4081,36.0,48.0
99,1.4195,38.0,44.0
100,1.2628,30.0,48.0
