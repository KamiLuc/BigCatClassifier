batch_size: 16

learning_rate: 0.0001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3119,16.0,14.0
2,2.2784,10.0,20.0
3,2.2531,12.0,18.0
4,2.3503,14.0,10.0
5,2.2808,8.0,10.0
6,2.3384,12.0,12.0
7,2.2777,12.0,18.0
8,2.2556,14.0,18.0
9,2.2890,14.0,12.0
10,2.2522,14.0,14.0
11,2.2468,24.0,22.0
12,2.3426,22.0,22.0
13,2.2025,32.0,20.0
14,2.2028,26.0,22.0
15,2.1311,28.0,24.0
16,2.1187,26.0,22.0
17,1.8868,24.0,24.0
18,2.3296,28.0,24.0
19,2.1434,34.0,24.0
20,2.4537,30.0,22.0
21,2.2240,34.0,24.0
22,1.9092,30.0,26.0
23,1.7173,22.0,20.0
24,2.2434,32.0,26.0
25,2.0935,26.0,22.0
26,1.6404,26.0,28.0
27,1.9839,30.0,30.0
28,2.2670,32.0,36.0
29,1.5003,34.0,34.0
30,2.0727,36.0,36.0
31,2.2724,28.0,34.0
32,1.9372,24.0,32.0
33,1.9347,34.0,34.0
34,1.4057,32.0,38.0
35,1.4516,36.0,38.0
36,0.9839,34.0,36.0
37,1.7238,32.0,34.0
38,1.7327,34.0,38.0
39,2.3661,40.0,38.0
40,1.9354,34.0,42.0
41,1.6051,34.0,34.0
42,1.6441,36.0,40.0
43,1.2828,30.0,42.0
44,1.6658,24.0,38.0
45,2.2106,34.0,38.0
46,1.2750,42.0,48.0
47,1.1501,28.0,38.0
48,1.2215,38.0,48.0
49,0.9975,42.0,40.0
50,0.9870,32.0,42.0
51,1.0666,36.0,42.0
52,1.2268,32.0,46.0
53,1.8999,32.0,44.0
54,0.9396,30.0,40.0
55,1.1054,34.0,50.0
56,1.4625,40.0,44.0
57,1.1406,32.0,42.0
58,1.9566,38.0,46.0
59,1.6305,44.0,48.0
60,1.5463,38.0,52.0
61,1.4129,34.0,48.0
62,2.2240,32.0,50.0
63,1.3760,40.0,50.0
64,1.0949,40.0,46.0
65,2.1914,32.0,46.0
66,1.7107,38.0,52.0
67,0.7151,44.0,46.0
68,0.7677,38.0,42.0
69,1.4954,44.0,50.0
70,1.6810,46.0,52.0
71,0.8663,46.0,50.0
72,0.9085,42.0,54.0
73,1.7406,38.0,42.0
74,0.8709,50.0,52.0
75,1.4061,46.0,50.0
76,0.5460,48.0,54.0
77,1.5007,46.0,50.0
78,1.1858,44.0,44.0
79,1.3771,38.0,48.0
80,1.9682,36.0,48.0
81,1.6070,36.0,46.0
82,1.3851,50.0,44.0
83,1.4174,50.0,46.0
84,1.5279,52.0,48.0
85,1.3174,44.0,46.0
86,1.3217,42.0,48.0
87,0.9016,52.0,48.0
88,1.5499,44.0,52.0
89,2.0292,44.0,50.0
90,1.4766,52.0,52.0
91,1.0719,46.0,50.0
92,1.0850,50.0,48.0
93,1.6203,46.0,46.0
94,0.8769,46.0,52.0
95,1.3536,44.0,40.0
96,0.8141,58.0,52.0
97,1.5682,54.0,50.0
98,1.1059,52.0,46.0
99,0.9701,50.0,50.0
100,1.5569,56.0,48.0
