batch_size: 32

learning_rate: 0.01

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3094,10.0,10.0
2,2.3046,10.0,10.0
3,2.3051,10.0,10.0
4,2.3020,10.0,10.0
5,2.2884,10.0,10.0
6,2.3077,10.0,10.0
7,2.3130,10.0,10.0
8,2.3162,10.0,10.0
9,2.3175,10.0,10.0
10,2.2951,10.0,10.0
11,2.2868,10.0,10.0
12,2.3169,10.0,10.0
13,2.3018,10.0,10.0
14,2.3097,10.0,10.0
15,2.3089,10.0,10.0
16,2.3039,10.0,10.0
17,2.3022,10.0,10.0
18,2.2882,10.0,10.0
19,2.3126,10.0,10.0
20,2.2954,10.0,10.0
21,2.3019,10.0,10.0
22,2.2992,10.0,10.0
23,2.3068,10.0,10.0
24,2.3150,10.0,10.0
25,2.3137,10.0,10.0
26,2.2985,10.0,10.0
27,2.2951,10.0,10.0
28,2.3204,10.0,10.0
29,2.3020,10.0,10.0
30,2.3012,10.0,10.0
31,2.2920,10.0,10.0
32,2.3150,10.0,10.0
33,2.3012,10.0,10.0
34,2.3276,10.0,10.0
35,2.3121,10.0,10.0
36,2.3254,10.0,10.0
37,2.2966,10.0,10.0
38,2.3074,10.0,10.0
39,2.3143,10.0,10.0
40,2.3016,10.0,10.0
41,2.3261,10.0,10.0
42,2.3095,10.0,10.0
43,2.3156,10.0,10.0
44,2.3054,10.0,10.0
45,2.3138,10.0,10.0
46,2.3055,10.0,10.0
47,2.2954,10.0,10.0
48,2.2943,10.0,10.0
49,2.2883,10.0,10.0
50,2.2756,10.0,10.0
51,2.2967,10.0,10.0
52,2.2993,10.0,10.0
53,2.3082,10.0,10.0
54,2.3112,10.0,10.0
55,2.3190,10.0,10.0
56,2.3239,10.0,10.0
57,2.2994,10.0,10.0
58,2.3040,10.0,10.0
59,2.3182,10.0,10.0
60,2.3024,10.0,10.0
61,2.3119,10.0,10.0
62,2.3117,10.0,10.0
63,2.3051,10.0,10.0
64,2.2973,10.0,10.0
65,2.3244,10.0,10.0
66,2.3113,10.0,10.0
67,2.3041,10.0,10.0
68,2.3066,10.0,10.0
69,2.2907,10.0,10.0
70,2.3170,10.0,10.0
71,2.2962,10.0,10.0
72,2.2916,10.0,10.0
73,2.3139,10.0,10.0
74,2.3197,10.0,10.0
75,2.3037,10.0,10.0
76,2.2983,10.0,10.0
77,2.3059,10.0,10.0
78,2.3025,10.0,10.0
79,2.2817,10.0,10.0
80,2.3019,10.0,10.0
81,2.3314,10.0,10.0
82,2.3277,10.0,10.0
83,2.2966,10.0,10.0
84,2.2959,10.0,10.0
85,2.3283,10.0,10.0
86,2.3277,10.0,10.0
87,2.3059,10.0,10.0
88,2.3288,10.0,10.0
89,2.3094,10.0,10.0
90,2.2848,10.0,10.0
91,2.3177,10.0,10.0
92,2.3203,10.0,10.0
93,2.2890,10.0,10.0
94,2.2980,10.0,10.0
95,2.3022,10.0,10.0
96,2.3151,10.0,10.0
97,2.3215,10.0,10.0
98,2.2731,10.0,10.0
99,2.3149,10.0,10.0
100,2.3064,10.0,10.0
