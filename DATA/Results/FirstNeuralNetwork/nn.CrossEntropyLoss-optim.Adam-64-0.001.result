batch_size: 64

learning_rate: 0.001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.1523,12.0,22.0
2,1.8052,20.0,26.0
3,1.9098,18.0,34.0
4,1.8713,20.0,38.0
5,1.6231,30.0,38.0
6,1.6156,40.0,44.0
7,1.4113,42.0,50.0
8,1.4581,44.0,48.0
9,1.4452,38.0,42.0
10,1.6149,32.0,42.0
11,1.4991,46.0,48.0
12,1.1314,48.0,54.0
13,1.6750,58.0,48.0
14,1.3662,56.0,50.0
15,1.1414,54.0,54.0
16,1.3356,54.0,56.0
17,1.1883,62.0,52.0
18,1.4763,60.0,50.0
19,0.8609,64.0,50.0
20,1.2481,64.0,44.0
21,0.9036,68.0,46.0
22,0.7219,68.0,50.0
23,0.7146,62.0,58.0
24,0.8077,62.0,52.0
25,0.8186,66.0,58.0
26,0.7952,68.0,54.0
27,0.9619,66.0,62.0
28,0.7909,68.0,54.0
29,0.7977,64.0,56.0
30,0.4727,72.0,58.0
31,0.6720,72.0,56.0
32,0.4902,76.0,52.0
33,0.7847,74.0,50.0
34,0.5505,74.0,54.0
35,0.6826,74.0,54.0
36,0.5198,76.0,50.0
37,0.3656,74.0,52.0
38,0.3230,68.0,56.0
39,0.5439,74.0,54.0
40,0.5445,68.0,52.0
41,0.4685,72.0,56.0
42,0.4791,68.0,52.0
43,0.3245,66.0,54.0
44,0.5182,68.0,58.0
45,0.5415,72.0,52.0
46,0.3005,74.0,60.0
47,0.2837,66.0,46.0
48,0.4046,72.0,56.0
49,0.5059,70.0,60.0
50,0.1992,70.0,60.0
51,0.6142,72.0,58.0
52,0.2788,68.0,58.0
53,0.3992,68.0,52.0
54,0.3151,68.0,58.0
55,0.2099,62.0,60.0
56,0.3529,64.0,56.0
57,0.2958,62.0,64.0
58,0.3084,74.0,58.0
59,0.2754,70.0,58.0
60,0.1441,72.0,58.0
61,0.1796,72.0,64.0
62,0.0944,66.0,62.0
63,0.1657,66.0,64.0
64,0.2688,70.0,58.0
65,0.2980,76.0,54.0
66,0.3196,74.0,62.0
67,0.2025,68.0,52.0
68,0.1049,66.0,58.0
69,0.1843,74.0,58.0
70,0.2132,70.0,64.0
71,0.2623,70.0,60.0
72,0.3403,74.0,64.0
73,0.2318,68.0,60.0
74,0.3235,72.0,62.0
75,0.1833,70.0,58.0
76,0.2926,72.0,62.0
77,0.1305,70.0,60.0
78,0.1938,74.0,58.0
79,0.1661,72.0,68.0
80,0.2518,76.0,64.0
81,0.4263,68.0,62.0
82,0.2441,66.0,58.0
83,0.1429,68.0,62.0
84,0.2706,70.0,66.0
85,0.1232,68.0,56.0
86,0.1273,68.0,58.0
87,0.1283,72.0,64.0
88,0.1974,70.0,66.0
89,0.1943,64.0,60.0
90,0.2307,72.0,54.0
91,0.0720,72.0,62.0
92,0.0459,66.0,62.0
93,0.0987,68.0,62.0
94,0.0505,68.0,58.0
95,0.2010,68.0,58.0
96,0.2005,66.0,70.0
97,0.0644,70.0,64.0
98,0.1034,68.0,54.0
99,0.1502,70.0,58.0
100,0.0939,70.0,58.0
