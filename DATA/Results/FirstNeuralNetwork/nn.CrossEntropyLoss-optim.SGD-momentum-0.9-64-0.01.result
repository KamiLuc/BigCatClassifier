batch_size: 64

learning_rate: 0.01

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.2652,18.0,22.0
2,2.2125,16.0,22.0
3,1.9796,20.0,26.0
4,1.8743,22.0,34.0
5,1.6295,22.0,32.0
6,1.7206,30.0,40.0
7,1.5549,32.0,44.0
8,1.7279,42.0,42.0
9,1.4663,44.0,42.0
10,1.8552,30.0,46.0
11,1.3747,34.0,34.0
12,1.7105,34.0,46.0
13,1.6451,48.0,50.0
14,1.7034,48.0,44.0
15,1.1337,58.0,54.0
16,0.9773,48.0,54.0
17,1.1093,54.0,46.0
18,1.1556,58.0,48.0
19,1.1167,52.0,52.0
20,0.7909,50.0,50.0
21,1.0245,52.0,42.0
22,0.8827,54.0,46.0
23,1.1300,56.0,52.0
24,0.8369,56.0,42.0
25,0.9092,62.0,56.0
26,1.1945,60.0,56.0
27,0.6029,62.0,60.0
28,0.6133,58.0,56.0
29,0.8424,64.0,52.0
30,0.5516,60.0,56.0
31,0.7322,68.0,58.0
32,0.8446,60.0,54.0
33,0.6425,66.0,48.0
34,0.9122,68.0,54.0
35,0.7151,60.0,52.0
36,0.7082,66.0,52.0
37,0.7476,68.0,54.0
38,0.4869,68.0,46.0
39,0.6126,60.0,54.0
40,0.5559,66.0,60.0
41,0.5368,64.0,52.0
42,0.3355,64.0,56.0
43,0.4906,68.0,60.0
44,0.5264,60.0,54.0
45,0.4483,66.0,58.0
46,0.6135,66.0,60.0
47,0.4076,72.0,62.0
48,0.3649,66.0,58.0
49,0.4685,62.0,56.0
50,0.3692,58.0,56.0
51,0.2621,54.0,52.0
52,0.3960,58.0,58.0
53,0.4149,60.0,54.0
54,0.3459,62.0,52.0
55,0.1305,72.0,52.0
56,0.1895,70.0,60.0
57,0.3433,62.0,56.0
58,0.5957,60.0,64.0
59,0.1705,62.0,48.0
60,0.2504,70.0,56.0
61,0.1048,72.0,58.0
62,0.2341,66.0,50.0
63,0.2438,68.0,58.0
64,0.2158,72.0,58.0
65,0.2837,74.0,62.0
66,0.4913,66.0,64.0
67,0.1271,60.0,56.0
68,0.1817,60.0,60.0
69,0.2409,68.0,58.0
70,0.1879,68.0,60.0
71,0.1642,70.0,56.0
72,0.0939,66.0,60.0
73,0.0933,64.0,48.0
74,0.1610,60.0,60.0
75,0.3535,64.0,52.0
76,0.4738,70.0,54.0
77,0.0476,68.0,54.0
78,0.0986,56.0,60.0
79,0.1915,70.0,60.0
80,0.2116,70.0,58.0
81,0.2917,62.0,58.0
82,0.2077,70.0,56.0
83,0.2468,60.0,58.0
84,0.0925,66.0,60.0
85,0.0507,56.0,62.0
86,0.6004,64.0,58.0
87,0.1467,68.0,54.0
88,0.1946,68.0,56.0
89,0.1707,72.0,52.0
90,0.2010,62.0,60.0
91,0.1413,66.0,62.0
92,0.1056,66.0,64.0
93,0.1481,72.0,58.0
94,0.2487,62.0,60.0
95,0.1254,64.0,58.0
96,0.0498,68.0,58.0
97,0.1711,66.0,58.0
98,0.0832,68.0,52.0
99,0.0285,66.0,58.0
100,0.0453,58.0,60.0
