batch_size: 32

learning_rate: 0.001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.1590,18.0,18.0
2,1.7463,16.0,32.0
3,2.1382,22.0,28.0
4,1.9732,18.0,28.0
5,2.1876,16.0,34.0
6,1.3171,36.0,34.0
7,1.6411,36.0,42.0
8,0.7700,28.0,38.0
9,0.9511,36.0,48.0
10,0.9249,44.0,48.0
11,0.8643,64.0,42.0
12,1.2469,66.0,48.0
13,1.0524,44.0,44.0
14,0.8559,62.0,54.0
15,1.0859,62.0,50.0
16,1.1150,48.0,48.0
17,0.3002,50.0,48.0
18,0.5176,56.0,58.0
19,0.1653,58.0,58.0
20,1.5741,56.0,50.0
21,0.5532,54.0,58.0
22,1.4661,52.0,58.0
23,0.8575,56.0,54.0
24,2.0385,44.0,50.0
25,0.5473,56.0,46.0
26,0.7997,50.0,52.0
27,0.4184,56.0,54.0
28,0.4602,56.0,52.0
29,0.1068,56.0,42.0
30,2.0066,66.0,54.0
31,0.3792,50.0,52.0
32,0.3715,50.0,48.0
33,1.3934,60.0,44.0
34,0.9808,52.0,54.0
35,0.5386,62.0,46.0
36,1.4904,42.0,50.0
37,2.4492,62.0,54.0
38,0.1108,54.0,44.0
39,1.5046,44.0,44.0
40,1.4274,50.0,48.0
41,0.0741,46.0,50.0
42,1.9839,50.0,52.0
43,0.5816,56.0,56.0
44,0.4777,60.0,52.0
45,0.6897,52.0,48.0
46,0.1373,60.0,58.0
47,2.3895,44.0,46.0
48,0.8052,56.0,50.0
49,0.0499,52.0,50.0
50,0.2607,56.0,52.0
51,0.0808,54.0,54.0
52,0.2345,56.0,48.0
53,0.0810,56.0,48.0
54,0.0856,44.0,50.0
55,0.4739,56.0,50.0
56,0.5230,58.0,44.0
57,0.5842,54.0,50.0
58,1.0738,46.0,48.0
59,0.8670,60.0,44.0
60,0.3759,56.0,56.0
61,0.6820,58.0,50.0
62,0.3083,52.0,46.0
63,0.8995,60.0,44.0
64,0.1413,60.0,52.0
65,0.0204,62.0,48.0
66,0.0151,62.0,44.0
67,0.3585,56.0,52.0
68,0.0787,54.0,52.0
69,0.1441,60.0,50.0
70,0.9481,66.0,50.0
71,0.1811,54.0,48.0
72,0.5846,46.0,54.0
73,0.1579,56.0,52.0
74,0.3070,66.0,46.0
75,0.2910,58.0,54.0
76,0.1790,62.0,46.0
77,0.2535,52.0,44.0
78,1.0353,60.0,50.0
79,0.2313,60.0,50.0
80,0.0401,62.0,48.0
81,0.3058,64.0,50.0
82,0.0886,56.0,54.0
83,0.3583,56.0,50.0
84,0.0214,56.0,48.0
85,0.0181,56.0,54.0
86,0.0668,54.0,54.0
87,0.0522,58.0,54.0
88,0.0140,58.0,44.0
89,0.0845,60.0,54.0
90,0.8134,54.0,54.0
91,0.5087,56.0,42.0
92,0.7691,56.0,50.0
93,1.3941,54.0,48.0
94,0.2805,52.0,54.0
95,0.1663,54.0,42.0
96,0.5101,58.0,44.0
97,0.2188,54.0,46.0
98,0.3035,52.0,48.0
99,0.3974,58.0,50.0
100,0.3622,50.0,48.0
