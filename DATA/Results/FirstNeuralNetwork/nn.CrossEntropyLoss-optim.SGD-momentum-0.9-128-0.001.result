batch_size: 128

learning_rate: 0.001

model: BigCatClassifier(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu1): ReLU()
  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu2): ReLU()
  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=100352, out_features=256, bias=True)
  (relu3): ReLU()
  (dropout): Dropout(p=0.5, inplace=False)
  (fc2): Linear(in_features=256, out_features=10, bias=True)
) 

begin training

epoch,loss,test accuracy,valid accuracy
1,2.3144,10.0,12.0
2,2.2980,16.0,14.0
3,2.2914,10.0,12.0
4,2.2591,16.0,26.0
5,2.2756,14.0,26.0
6,2.2589,14.0,16.0
7,2.2447,16.0,24.0
8,2.2750,16.0,24.0
9,2.2664,20.0,18.0
10,2.2751,14.0,26.0
11,2.2191,16.0,26.0
12,2.1840,16.0,34.0
13,2.1228,18.0,22.0
14,2.1375,18.0,28.0
15,2.0291,14.0,26.0
16,2.1362,18.0,32.0
17,1.9393,14.0,28.0
18,1.8786,20.0,30.0
19,1.8506,22.0,30.0
20,1.9482,18.0,32.0
21,2.0007,22.0,34.0
22,1.8426,28.0,38.0
23,1.6285,18.0,28.0
24,1.7525,24.0,38.0
25,1.8491,18.0,42.0
26,1.7267,16.0,38.0
27,1.8706,32.0,42.0
28,1.7927,32.0,46.0
29,1.8254,28.0,44.0
30,1.7127,30.0,38.0
31,1.7585,28.0,36.0
32,1.5631,26.0,44.0
33,1.8026,20.0,38.0
34,1.8841,22.0,44.0
35,1.4399,38.0,44.0
36,1.3646,26.0,34.0
37,1.6926,20.0,36.0
38,1.6973,34.0,36.0
39,1.6561,22.0,42.0
40,1.4293,24.0,40.0
41,1.5209,26.0,38.0
42,1.4462,32.0,48.0
43,1.6498,38.0,50.0
44,1.3852,28.0,46.0
45,1.6222,26.0,44.0
46,1.4040,22.0,42.0
47,1.5250,30.0,38.0
48,1.5377,40.0,46.0
49,1.3886,24.0,46.0
50,1.5662,40.0,50.0
51,1.6732,30.0,48.0
52,1.6130,36.0,40.0
53,1.3238,38.0,42.0
54,1.3889,32.0,46.0
55,1.2127,32.0,40.0
56,1.2679,38.0,44.0
57,1.3537,36.0,48.0
58,1.0954,38.0,46.0
59,1.3947,38.0,44.0
60,1.4226,32.0,38.0
61,1.6293,34.0,44.0
62,1.5107,34.0,48.0
63,1.3931,36.0,44.0
64,1.4294,34.0,40.0
65,1.2794,40.0,48.0
66,1.2785,34.0,48.0
67,1.6178,44.0,36.0
68,1.2565,42.0,42.0
69,1.4712,44.0,44.0
70,1.3072,40.0,40.0
71,1.3292,48.0,46.0
72,1.1826,38.0,50.0
73,0.9543,34.0,46.0
74,1.1238,44.0,46.0
75,1.1262,46.0,50.0
76,1.1204,48.0,46.0
77,1.3888,40.0,46.0
78,1.2328,40.0,46.0
79,1.3010,48.0,46.0
80,1.1731,42.0,46.0
81,0.8541,48.0,42.0
82,1.3663,32.0,38.0
83,1.3105,50.0,50.0
84,1.3139,50.0,46.0
85,1.3318,48.0,46.0
86,1.0907,40.0,50.0
87,0.8230,46.0,54.0
88,1.1628,42.0,50.0
89,1.1657,44.0,52.0
90,1.3881,46.0,42.0
91,1.0245,46.0,48.0
92,1.0005,48.0,44.0
93,1.2010,54.0,50.0
94,1.2082,44.0,46.0
95,0.8142,50.0,48.0
96,1.1821,54.0,46.0
97,1.1309,56.0,46.0
98,1.0784,48.0,54.0
99,1.0319,48.0,48.0
100,1.1827,46.0,46.0
